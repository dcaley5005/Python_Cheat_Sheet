[
  {
    "objectID": "machine_learning_regression.html",
    "href": "machine_learning_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "machine_learning_regression.html#option-1-iloc",
    "href": "machine_learning_regression.html#option-1-iloc",
    "title": "Regression",
    "section": "Option 1: iloc",
    "text": "Option 1: iloc\n\n\nCode\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
  },
  {
    "objectID": "machine_learning_regression.html#option-2-not-iloc",
    "href": "machine_learning_regression.html#option-2-not-iloc",
    "title": "Regression",
    "section": "Option 2: Not iloc",
    "text": "Option 2: Not iloc\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n  dataset.drop(columns = 'PE'),\n  dataset.PE,\n  test_size = 0.25, \n  random_state = 0)"
  },
  {
    "objectID": "machine_learning_regression.html#prediction",
    "href": "machine_learning_regression.html#prediction",
    "title": "Regression",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\ny_pred = regressor.predict(X_test)\ny_pred[:5]\n\n\narray([431.43458966, 458.57522935, 462.76604909, 448.60649816,\n       457.87605198])"
  },
  {
    "objectID": "machine_learning_regression.html#model-performance",
    "href": "machine_learning_regression.html#model-performance",
    "title": "Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\n\nCode\ndataset['PE'].describe()\n\n\ncount    9568.000000\nmean      454.365009\nstd        17.066995\nmin       420.260000\n25%       439.750000\n50%       451.550000\n75%       468.430000\nmax       495.760000\nName: PE, dtype: float64\n\n\n\n\nCode\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 93.24%\nMSE Score: 19.83\nRMSE Score: 4.45\nRMSE Score: 4.45\nMAE Score: 3.56"
  },
  {
    "objectID": "machine_learning_regression.html#predicition",
    "href": "machine_learning_regression.html#predicition",
    "title": "Regression",
    "section": "Predicition",
    "text": "Predicition\n\n\nCode\ny_pred = regressor.predict(poly_reg.transform(X_test))\nnp.set_printoptions(precision=2)"
  },
  {
    "objectID": "machine_learning_regression.html#model-performance-1",
    "href": "machine_learning_regression.html#model-performance-1",
    "title": "Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\n\nCode\ndataset['AT'].describe()\n\n\ncount    9568.000000\nmean       19.651231\nstd         7.452473\nmin         1.810000\n25%        13.510000\n50%        20.345000\n75%        25.720000\nmax        37.110000\nName: AT, dtype: float64\n\n\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 94.60%\nMSE Score: 15.85\nRMSE Score: 3.98\nRMSE Score: 3.98\nMAE Score: 3.12"
  },
  {
    "objectID": "machine_learning_regression.html#prediction-1",
    "href": "machine_learning_regression.html#prediction-1",
    "title": "Regression",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)"
  },
  {
    "objectID": "machine_learning_regression.html#model-performance-2",
    "href": "machine_learning_regression.html#model-performance-2",
    "title": "Regression",
    "section": "Model Performance",
    "text": "Model Performance\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 96.09%\nMSE Score: 11.48\nRMSE Score: 3.39\nRMSE Score: 3.39\nMAE Score: 2.50"
  },
  {
    "objectID": "machine_learning_regression.html#feature-scaling",
    "href": "machine_learning_regression.html#feature-scaling",
    "title": "Regression",
    "section": "Feature Scaling",
    "text": "Feature Scaling\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)"
  },
  {
    "objectID": "machine_learning_regression.html#inverse-transform",
    "href": "machine_learning_regression.html#inverse-transform",
    "title": "Regression",
    "section": "Inverse Transform",
    "text": "Inverse Transform\nThis un transforms the data\n\n\nCode\nprint(X_train[:1])\n\nX_train = sc.inverse_transform(X_train)\nX_test = sc.inverse_transform(X_test)\n\nprint(X_train[:1])\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n[[-0.73 -0.91  3.15 -0.25]]\n[[  14.2    42.86 1031.96   69.59]]"
  },
  {
    "objectID": "machine_learning_regression.html#linear-kernal",
    "href": "machine_learning_regression.html#linear-kernal",
    "title": "Regression",
    "section": "Linear Kernal",
    "text": "Linear Kernal\n\n\nCode\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'linear')\nregressor.fit(X_train, y_train)\n\n\nSVR(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVRSVR(kernel='linear')\n\n\n\nPrediction\n\n\nCode\ny_pred = regressor.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 93.20%\nMSE Score: 19.94\nRMSE Score: 4.47\nRMSE Score: 4.47\nMAE Score: 3.55"
  },
  {
    "objectID": "machine_learning_regression.html#polynomial-kernal",
    "href": "machine_learning_regression.html#polynomial-kernal",
    "title": "Regression",
    "section": "Polynomial Kernal",
    "text": "Polynomial Kernal\n\n\nCode\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'poly')\nregressor.fit(X_train, y_train)\n\n\nSVR(kernel='poly')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVRSVR(kernel='poly')\n\n\n\nPrediction\n\n\nCode\ny_pred = regressor.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 78.31%\nMSE Score: 63.61\nRMSE Score: 7.98\nRMSE Score: 7.98\nMAE Score: 6.25"
  },
  {
    "objectID": "machine_learning_regression.html#radial-basis-function-rbf-kernal",
    "href": "machine_learning_regression.html#radial-basis-function-rbf-kernal",
    "title": "Regression",
    "section": "Radial Basis Function (RBF) Kernal",
    "text": "Radial Basis Function (RBF) Kernal\n\n\nCode\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, y_train)\n\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVRSVR()\n\n\n\nPrediction\n\n\nCode\ny_pred = regressor.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: 94.31%\nMSE Score: 16.68\nRMSE Score: 4.08\nRMSE Score: 4.08\nMAE Score: 3.14"
  },
  {
    "objectID": "machine_learning_regression.html#sigmoid-kernal",
    "href": "machine_learning_regression.html#sigmoid-kernal",
    "title": "Regression",
    "section": "Sigmoid Kernal",
    "text": "Sigmoid Kernal\n\n\nCode\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'sigmoid')\nregressor.fit(X_train, y_train)\n\n\nSVR(kernel='sigmoid')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVRSVR(kernel='sigmoid')\n\n\n\nPrediction\n\n\nCode\ny_pred = regressor.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\nprint(\"R2 Score: {:.2f}%\".format(r2_score(y_test, y_pred)*100))\nprint(\"MSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred)))\nprint(\"RMSE Score: {:.2f}\".format(mean_squared_error(y_test, y_pred, squared=False)))\nprint(\"RMSE Score: {:.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"MAE Score: {:.2f}\".format(mean_absolute_error(y_test, y_pred)))\n\n\nR2 Score: -7452.15%\nMSE Score: 22149.23\nRMSE Score: 148.83\nRMSE Score: 148.83\nMAE Score: 113.29"
  },
  {
    "objectID": "machine_learning_regression.html#mae",
    "href": "machine_learning_regression.html#mae",
    "title": "Regression",
    "section": "MAE",
    "text": "MAE\n\n\nCode\nscores = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10, scoring='neg_mean_absolute_error')\nprint(\"MAE Score: {:.2f}\".format(-scores.mean()))\nprint(\"Standard Deviation: {:.2f}\".format(scores.std()))\n\n\nMAE Score: 3.20\nStandard Deviation: 0.10"
  },
  {
    "objectID": "machine_learning_regression.html#mse",
    "href": "machine_learning_regression.html#mse",
    "title": "Regression",
    "section": "MSE",
    "text": "MSE\n\n\nCode\nscores = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10, scoring='neg_mean_squared_error')\nprint(\"MSE Score: {:.2f}\".format(-scores.mean()))\nprint(\"Standard Deviation: {:.2f}\".format(scores.std()))\n\n\nMSE Score: 17.92\nStandard Deviation: 1.50"
  },
  {
    "objectID": "machine_learning_regression.html#r2-score",
    "href": "machine_learning_regression.html#r2-score",
    "title": "Regression",
    "section": "R2 Score",
    "text": "R2 Score\n\n\nCode\nscores = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10, scoring='r2')\nprint(\"R-Squared Score: {:.2f}%\".format(scores.mean()*100))\nprint(\"Standard Deviation: {:.2f}%\".format(scores.std()*100))\n\n\nR-Squared Score: 93.81%\nStandard Deviation: 0.53%"
  },
  {
    "objectID": "machine_learning_regression.html#applying-results",
    "href": "machine_learning_regression.html#applying-results",
    "title": "Regression",
    "section": "Applying Results",
    "text": "Applying Results\n\nOption 1\n\n\nCode\nbest_svr = grid_search.best_estimator_\nbest_svr.fit(X_train,y_train)\n\ny_pred = best_svr.predict(X_test)\n\ny_pred[:5]\n\n\narray([433.85, 457.77, 461.75, 448.75, 457.67])\n\n\n\n\nOption 2\n\n\nCode\nregressor = SVR(kernel = 'rbf', C = 1, gamma = 0.4)\nregressor.fit(X_train, y_train)\n\ny_pred = regressor.predict(X_test)\n\ny_pred[:5]\n\n\narray([433.85, 457.77, 461.75, 448.75, 457.67])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Transformation",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This\n\n\nCode\n```{python}\nprint(\"test\")\n```\n\n\ntest\n\n\n\n\nCode\n```{python}\n1 + 1\n```\n\n\n2"
  },
  {
    "objectID": "data_transformation.html",
    "href": "data_transformation.html",
    "title": "Data Transformation",
    "section": "",
    "text": "This is a Data Transformation cheat sheet meant to be a second brain when performing data science functions."
  },
  {
    "objectID": "data_transformation.html#reading-csv",
    "href": "data_transformation.html#reading-csv",
    "title": "Data Transformation",
    "section": "Reading CSV",
    "text": "Reading CSV\n\n\nCode\nsales = pd.read_csv(\n    'data/sales_data.csv',\n    parse_dates=['Date']\n)\n\nsales = sales.sort_values(by=['Date'], ascending = True)"
  },
  {
    "objectID": "data_transformation.html#adding-column-names",
    "href": "data_transformation.html#adding-column-names",
    "title": "Data Transformation",
    "section": "Adding Column Names",
    "text": "Adding Column Names\n\n\nCode\nmarvel_data = [\n    ['Spider-Man', 'male', 1962],\n    ['Captain America', 'male', 1941],\n    ['Wolverine', 'male', 1974],\n    ['Iron Man', 'male', 1963],\n    ['Thor', 'male', 1963],\n    ['Thing', 'male', 1961],\n    ['Mister Fantastic', 'male', 1961],\n    ['Hulk', 'male', 1962],\n    ['Beast', 'male', 1963],\n    ['Invisible Woman', 'female', 1961],\n    ['Storm', 'female', 1975],\n    ['Namor', 'male', 1939],\n    ['Hawkeye', 'male', 1964],\n    ['Daredevil', 'male', 1964],\n    ['Doctor Strange', 'male', 1963],\n    ['Hank Pym', 'male', 1962],\n    ['Scarlet Witch', 'female', 1964],\n    ['Wasp', 'female', 1963],\n    ['Black Widow', 'female', 1964],\n    ['Vision', 'male', 1968]\n]\n\nmarvel_df = pd.DataFrame(data=marvel_data,\n                         columns=['name', 'sex', 'first_appearance'])\nmarvel_df\n\n\n\n\n\n\n  \n    \n      \n      name\n      sex\n      first_appearance\n    \n  \n  \n    \n      0\n      Spider-Man\n      male\n      1962\n    \n    \n      1\n      Captain America\n      male\n      1941\n    \n    \n      2\n      Wolverine\n      male\n      1974\n    \n    \n      3\n      Iron Man\n      male\n      1963\n    \n    \n      4\n      Thor\n      male\n      1963\n    \n    \n      5\n      Thing\n      male\n      1961\n    \n    \n      6\n      Mister Fantastic\n      male\n      1961\n    \n    \n      7\n      Hulk\n      male\n      1962\n    \n    \n      8\n      Beast\n      male\n      1963\n    \n    \n      9\n      Invisible Woman\n      female\n      1961\n    \n    \n      10\n      Storm\n      female\n      1975\n    \n    \n      11\n      Namor\n      male\n      1939\n    \n    \n      12\n      Hawkeye\n      male\n      1964\n    \n    \n      13\n      Daredevil\n      male\n      1964\n    \n    \n      14\n      Doctor Strange\n      male\n      1963\n    \n    \n      15\n      Hank Pym\n      male\n      1962\n    \n    \n      16\n      Scarlet Witch\n      female\n      1964\n    \n    \n      17\n      Wasp\n      female\n      1963\n    \n    \n      18\n      Black Widow\n      female\n      1964\n    \n    \n      19\n      Vision\n      male\n      1968"
  },
  {
    "objectID": "data_transformation.html#adding-column-names-when-reading-csv",
    "href": "data_transformation.html#adding-column-names-when-reading-csv",
    "title": "Data Transformation",
    "section": "Adding Column Names When Reading CSV",
    "text": "Adding Column Names When Reading CSV\n\n\nCode\nbtc_price = pd.read_csv(\n    'data/btc-market-price.csv',\n    header=None,\n    names=['Timestamp', 'Price'],\n    index_col=0, # this sets the column in this instance its the first column 1\n    # index_col='Timestamp',  # You can also add the column name\n    parse_dates=True\n)\n\nbtc_price.head()\n\n\n\n\n\n\n  \n    \n      \n      Price\n    \n    \n      Timestamp\n      \n    \n  \n  \n    \n      2017-04-02\n      1099.169125\n    \n    \n      2017-04-03\n      1141.813000\n    \n    \n      2017-04-04\n      1141.600363\n    \n    \n      2017-04-05\n      1133.079314\n    \n    \n      2017-04-06\n      1196.307937"
  },
  {
    "objectID": "data_transformation.html#setting-the-index",
    "href": "data_transformation.html#setting-the-index",
    "title": "Data Transformation",
    "section": "Setting the Index",
    "text": "Setting the Index\n\n\nCode\ncountry_stat.index = [\n    'Canada',\n    'France',\n    'Germany',\n    'Italy',\n    'Japan',\n    'United Kingdom',\n    'United States',\n]\n\ncountry_stat.sort_values(by=['Continent','Population','HDI'], ascending=False)\n\n\n\n\n\n\n  \n    \n      \n      Population\n      GDP\n      Surface Area\n      HDI\n      Continent\n    \n  \n  \n    \n      Germany\n      80.940\n      3874437\n      357114\n      0.916\n      Europe\n    \n    \n      United Kingdom\n      64.511\n      2950039\n      242495\n      0.907\n      Europe\n    \n    \n      France\n      63.951\n      2833687\n      640679\n      0.888\n      Europe\n    \n    \n      Italy\n      60.665\n      2167744\n      301336\n      0.873\n      Europe\n    \n    \n      Japan\n      127.061\n      4602367\n      377930\n      0.891\n      Asia\n    \n    \n      United States\n      318.523\n      17348075\n      9525067\n      0.915\n      America\n    \n    \n      Canada\n      35.467\n      1785387\n      9984670\n      0.913\n      America"
  },
  {
    "objectID": "data_transformation.html#splitting-string-into-two-columns",
    "href": "data_transformation.html#splitting-string-into-two-columns",
    "title": "Data Transformation",
    "section": "Splitting String into Two Columns",
    "text": "Splitting String into Two Columns\nIn this example we split the county and state from each other\n\n\nCode\ncensus_county = pd.read_csv(\"data/census_county.csv\")\n\ncensus_county[[\"state\",\"county\",\"NAME\"]].head()\n\n\nnew = census_county[\"NAME\"].str.split(\", \", expand = True)\ncensus_county[\"county_name\"] = new[0]\ncensus_county[\"state_name\"] = new[1]\n\ncensus_county.drop(columns=['NAME'], inplace=True)\ncensus_county.head()\n\n\n\n\n\n\n  \n    \n      \n      state\n      county\n      median_income\n      population\n      year\n      county_name\n      state_name\n    \n  \n  \n    \n      0\n      37\n      43\n      36711.0\n      10506\n      2011\n      Clay County\n      North Carolina\n    \n    \n      1\n      37\n      51\n      44861.0\n      316478\n      2011\n      Cumberland County\n      North Carolina\n    \n    \n      2\n      37\n      81\n      46288.0\n      483081\n      2011\n      Guilford County\n      North Carolina\n    \n    \n      3\n      37\n      99\n      36826.0\n      39574\n      2011\n      Jackson County\n      North Carolina\n    \n    \n      4\n      37\n      139\n      45298.0\n      40511\n      2011\n      Pasquotank County\n      North Carolina"
  },
  {
    "objectID": "data_transformation.html#inspecting-datatypes",
    "href": "data_transformation.html#inspecting-datatypes",
    "title": "Data Transformation",
    "section": "Inspecting DataTypes",
    "text": "Inspecting DataTypes\n\n\nCode\nbtc_price.dtypes\n\n\nPrice    float64\ndtype: object"
  },
  {
    "objectID": "data_transformation.html#updating-string-using-to-date-time",
    "href": "data_transformation.html#updating-string-using-to-date-time",
    "title": "Data Transformation",
    "section": "Updating String Using to Date Time",
    "text": "Updating String Using to Date Time\n\n\nCode\nbtc_price = btc_price.reset_index()\nbtc_price['Timestamp'] = pd.to_datetime(btc_price['Timestamp'])\nbtc_price.dtypes\n\n\nTimestamp    datetime64[ns]\nPrice               float64\ndtype: object"
  },
  {
    "objectID": "data_transformation.html#filtering-data-time",
    "href": "data_transformation.html#filtering-data-time",
    "title": "Data Transformation",
    "section": "Filtering Data Time",
    "text": "Filtering Data Time\n\n\nCode\nbtc_price.set_index('Timestamp', inplace=True)\nbtc_price.loc['2017-09-29':'2017-10-05']\n\n\n\n\n\n\n  \n    \n      \n      Price\n    \n    \n      Timestamp\n      \n    \n  \n  \n    \n      2017-09-29\n      4193.574667\n    \n    \n      2017-09-30\n      4335.368317\n    \n    \n      2017-10-01\n      4360.722967\n    \n    \n      2017-10-02\n      4386.883750\n    \n    \n      2017-10-03\n      4293.306600\n    \n    \n      2017-10-04\n      4225.175000\n    \n    \n      2017-10-05\n      4338.852000"
  },
  {
    "objectID": "data_transformation.html#date-diff-calc",
    "href": "data_transformation.html#date-diff-calc",
    "title": "Data Transformation",
    "section": "Date Diff Calc",
    "text": "Date Diff Calc\n\n\nCode\nimport datetime\n\nyear = datetime.date.today().year\n\nmarvel_df['years_since'] = year - marvel_df['first_appearance']\nmarvel_df\n\n\n\n\n\n\n  \n    \n      \n      name\n      sex\n      first_appearance\n      years_since\n    \n  \n  \n    \n      0\n      Spider-Man\n      male\n      1962\n      61\n    \n    \n      1\n      Captain America\n      male\n      1941\n      82\n    \n    \n      2\n      Wolverine\n      male\n      1974\n      49\n    \n    \n      3\n      Iron Man\n      male\n      1963\n      60\n    \n    \n      4\n      Thor\n      male\n      1963\n      60\n    \n    \n      5\n      Thing\n      male\n      1961\n      62\n    \n    \n      6\n      Mister Fantastic\n      male\n      1961\n      62\n    \n    \n      7\n      Hulk\n      male\n      1962\n      61\n    \n    \n      8\n      Beast\n      male\n      1963\n      60\n    \n    \n      9\n      Invisible Woman\n      female\n      1961\n      62\n    \n    \n      10\n      Storm\n      female\n      1975\n      48\n    \n    \n      11\n      Namor\n      male\n      1939\n      84\n    \n    \n      12\n      Hawkeye\n      male\n      1964\n      59\n    \n    \n      13\n      Daredevil\n      male\n      1964\n      59\n    \n    \n      14\n      Doctor Strange\n      male\n      1963\n      60\n    \n    \n      15\n      Hank Pym\n      male\n      1962\n      61\n    \n    \n      16\n      Scarlet Witch\n      female\n      1964\n      59\n    \n    \n      17\n      Wasp\n      female\n      1963\n      60\n    \n    \n      18\n      Black Widow\n      female\n      1964\n      59\n    \n    \n      19\n      Vision\n      male\n      1968\n      55"
  },
  {
    "objectID": "data_transformation.html#convert-date-time-to-year-or-month",
    "href": "data_transformation.html#convert-date-time-to-year-or-month",
    "title": "Data Transformation",
    "section": "Convert Date Time To Year or Month",
    "text": "Convert Date Time To Year or Month\n\n\nCode\nsales['Year'] = sales['Date'].dt.year\nsales['Month'] = sales['Date'].dt.month\nsales\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Day\n      Month\n      Year\n      Customer_Age\n      Age_Group\n      Customer_Gender\n      Country\n      State\n      Product_Category\n      Sub_Category\n      Product\n      Order_Quantity\n      Unit_Cost\n      Unit_Price\n      Profit\n      Cost\n      Revenue\n    \n  \n  \n    \n      60993\n      2011-01-01\n      1\n      1\n      2011\n      42\n      Adults (35-64)\n      M\n      United States\n      California\n      Bikes\n      Road Bikes\n      Road-750 Black, 44\n      1\n      344\n      540\n      185\n      344\n      529\n    \n    \n      58729\n      2011-01-01\n      1\n      1\n      2011\n      33\n      Young Adults (25-34)\n      F\n      France\n      Yveline\n      Bikes\n      Road Bikes\n      Road-150 Red, 48\n      2\n      2171\n      3578\n      1097\n      4342\n      5439\n    \n    \n      58913\n      2011-01-01\n      1\n      1\n      2011\n      17\n      Youth (<25)\n      M\n      Canada\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-250 Red, 44\n      1\n      1519\n      2443\n      900\n      1519\n      2419\n    \n    \n      63711\n      2011-01-01\n      1\n      1\n      2011\n      39\n      Adults (35-64)\n      M\n      United States\n      Washington\n      Bikes\n      Road Bikes\n      Road-550-W Yellow, 38\n      3\n      713\n      1120\n      482\n      2139\n      2621\n    \n    \n      49787\n      2011-01-01\n      1\n      1\n      2011\n      23\n      Youth (<25)\n      M\n      Australia\n      Victoria\n      Bikes\n      Mountain Bikes\n      Mountain-200 Black, 46\n      1\n      1252\n      2295\n      561\n      1252\n      1813\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      22465\n      2016-07-31\n      31\n      7\n      2016\n      39\n      Adults (35-64)\n      F\n      United States\n      California\n      Accessories\n      Cleaners\n      Bike Wash - Dissolver\n      5\n      3\n      8\n      24\n      15\n      39\n    \n    \n      23363\n      2016-07-31\n      31\n      7\n      2016\n      39\n      Adults (35-64)\n      F\n      United States\n      California\n      Accessories\n      Fenders\n      Fender Set - Mountain\n      19\n      8\n      22\n      258\n      152\n      410\n    \n    \n      19225\n      2016-07-31\n      31\n      7\n      2016\n      48\n      Adults (35-64)\n      F\n      United Kingdom\n      England\n      Clothing\n      Caps\n      AWC Logo Cap\n      16\n      7\n      9\n      26\n      112\n      138\n    \n    \n      94147\n      2016-07-31\n      31\n      7\n      2016\n      51\n      Adults (35-64)\n      M\n      Germany\n      Saarland\n      Accessories\n      Tires and Tubes\n      ML Mountain Tire\n      6\n      11\n      30\n      82\n      66\n      148\n    \n    \n      80944\n      2016-07-31\n      31\n      7\n      2016\n      42\n      Adults (35-64)\n      F\n      United States\n      California\n      Accessories\n      Tires and Tubes\n      Patch Kit/8 Patches\n      7\n      1\n      2\n      7\n      7\n      14\n    \n  \n\n113036 rows × 18 columns"
  },
  {
    "objectID": "data_transformation.html#understanding-the-data",
    "href": "data_transformation.html#understanding-the-data",
    "title": "Data Transformation",
    "section": "Understanding the Data",
    "text": "Understanding the Data\n\n\nCode\nsales.shape\n\n\n(113036, 18)\n\n\n\n\nCode\nsales.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 113036 entries, 60993 to 80944\nData columns (total 18 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   Date              113036 non-null  datetime64[ns]\n 1   Day               113036 non-null  int64         \n 2   Month             113036 non-null  object        \n 3   Year              113036 non-null  int64         \n 4   Customer_Age      113036 non-null  int64         \n 5   Age_Group         113036 non-null  object        \n 6   Customer_Gender   113036 non-null  object        \n 7   Country           113036 non-null  object        \n 8   State             113036 non-null  object        \n 9   Product_Category  113036 non-null  object        \n 10  Sub_Category      113036 non-null  object        \n 11  Product           113036 non-null  object        \n 12  Order_Quantity    113036 non-null  int64         \n 13  Unit_Cost         113036 non-null  int64         \n 14  Unit_Price        113036 non-null  int64         \n 15  Profit            113036 non-null  int64         \n 16  Cost              113036 non-null  int64         \n 17  Revenue           113036 non-null  int64         \ndtypes: datetime64[ns](1), int64(9), object(8)\nmemory usage: 16.4+ MB\n\n\n\n\nCode\nsales.columns\n\n\nIndex(['Date', 'Day', 'Month', 'Year', 'Customer_Age', 'Age_Group',\n       'Customer_Gender', 'Country', 'State', 'Product_Category',\n       'Sub_Category', 'Product', 'Order_Quantity', 'Unit_Cost', 'Unit_Price',\n       'Profit', 'Cost', 'Revenue'],\n      dtype='object')"
  },
  {
    "objectID": "data_transformation.html#summary-statistics",
    "href": "data_transformation.html#summary-statistics",
    "title": "Data Transformation",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nCode\nsales.describe()\n\n\n\n\n\n\n  \n    \n      \n      Day\n      Year\n      Customer_Age\n      Order_Quantity\n      Unit_Cost\n      Unit_Price\n      Profit\n      Cost\n      Revenue\n    \n  \n  \n    \n      count\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n      113036.000000\n    \n    \n      mean\n      15.665753\n      2014.401739\n      35.919212\n      11.901660\n      267.296366\n      452.938427\n      285.051665\n      469.318695\n      754.370360\n    \n    \n      std\n      8.781567\n      1.272510\n      11.021936\n      9.561857\n      549.835483\n      922.071219\n      453.887443\n      884.866118\n      1309.094674\n    \n    \n      min\n      1.000000\n      2011.000000\n      17.000000\n      1.000000\n      1.000000\n      2.000000\n      -30.000000\n      1.000000\n      2.000000\n    \n    \n      25%\n      8.000000\n      2013.000000\n      28.000000\n      2.000000\n      2.000000\n      5.000000\n      29.000000\n      28.000000\n      63.000000\n    \n    \n      50%\n      16.000000\n      2014.000000\n      35.000000\n      10.000000\n      9.000000\n      24.000000\n      101.000000\n      108.000000\n      223.000000\n    \n    \n      75%\n      23.000000\n      2016.000000\n      43.000000\n      20.000000\n      42.000000\n      70.000000\n      358.000000\n      432.000000\n      800.000000\n    \n    \n      max\n      31.000000\n      2016.000000\n      87.000000\n      32.000000\n      2171.000000\n      3578.000000\n      15096.000000\n      42978.000000\n      58074.000000\n    \n  \n\n\n\n\n\n\nCode\nsales[['Age_Group','Unit_Price']].groupby('Age_Group').describe()\n\n\n\n\n\n\n  \n    \n      \n      Unit_Price\n    \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n    \n      Age_Group\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adults (35-64)\n      55824.0\n      455.185046\n      923.478763\n      2.0\n      5.0\n      25.0\n      70.0\n      3578.0\n    \n    \n      Seniors (64+)\n      730.0\n      156.260274\n      523.610077\n      2.0\n      5.0\n      22.0\n      35.0\n      3578.0\n    \n    \n      Young Adults (25-34)\n      38654.0\n      489.569928\n      946.438451\n      2.0\n      5.0\n      25.0\n      540.0\n      3578.0\n    \n    \n      Youth (<25)\n      17828.0\n      378.628674\n      868.912104\n      2.0\n      5.0\n      21.0\n      54.0\n      3578.0\n    \n  \n\n\n\n\n\n\nCode\n# Number of elements in the data\nsales.size\n\n\n2034648"
  },
  {
    "objectID": "data_transformation.html#data-types",
    "href": "data_transformation.html#data-types",
    "title": "Data Transformation",
    "section": "Data Types",
    "text": "Data Types\n\n\nCode\nsales.dtypes\n\n\nDate                datetime64[ns]\nDay                          int64\nMonth                       object\nYear                         int64\nCustomer_Age                 int64\nAge_Group                   object\nCustomer_Gender             object\nCountry                     object\nState                       object\nProduct_Category            object\nSub_Category                object\nProduct                     object\nOrder_Quantity               int64\nUnit_Cost                    int64\nUnit_Price                   int64\nProfit                       int64\nCost                         int64\nRevenue                      int64\ndtype: object\n\n\n\n\nCode\nsales['Unit_Cost'].describe()\n\n\ncount    113036.000000\nmean        267.296366\nstd         549.835483\nmin           1.000000\n25%           2.000000\n50%           9.000000\n75%          42.000000\nmax        2171.000000\nName: Unit_Cost, dtype: float64"
  },
  {
    "objectID": "data_transformation.html#individual-statistics",
    "href": "data_transformation.html#individual-statistics",
    "title": "Data Transformation",
    "section": "Individual Statistics",
    "text": "Individual Statistics\n\n\nCode\nsales['Unit_Cost'].mean()\n\n\n267.296365759581\n\n\n\n\nCode\nsales['Unit_Cost'].median()\n\n\n9.0\n\n\n\n\nCode\nsales['Unit_Cost'].min(), sales['Unit_Cost'].max()\n\n\n(1, 2171)\n\n\n\n\nCode\nsales['Unit_Cost'].std()\n\n\n549.8354831075126"
  },
  {
    "objectID": "data_transformation.html#quantiles",
    "href": "data_transformation.html#quantiles",
    "title": "Data Transformation",
    "section": "Quantiles",
    "text": "Quantiles\n\n\nCode\nsales['Unit_Cost'].quantile(0.25)\n\n\n2.0\n\n\n\n\nCode\nsales['Unit_Cost'].quantile([.2, .4, .6, .8, 1])\n\n\n0.2       2.0\n0.4       7.0\n0.6      13.0\n0.8     344.0\n1.0    2171.0\nName: Unit_Cost, dtype: float64"
  },
  {
    "objectID": "data_transformation.html#value-counts",
    "href": "data_transformation.html#value-counts",
    "title": "Data Transformation",
    "section": "Value Counts",
    "text": "Value Counts\n\n\nCode\nsales['Age_Group'].value_counts()\n\n\nAdults (35-64)          55824\nYoung Adults (25-34)    38654\nYouth (<25)             17828\nSeniors (64+)             730\nName: Age_Group, dtype: int64\n\n\n\n\nCode\n sales['Age_Group'].value_counts(normalize=True)\n\n\nAdults (35-64)          0.493860\nYoung Adults (25-34)    0.341962\nYouth (<25)             0.157720\nSeniors (64+)           0.006458\nName: Age_Group, dtype: float64\n\n\n\n\nCode\ncountry_group = sales.groupby(['Country'])\ncountry_group['Age_Group'].value_counts(normalize=True).loc['Australia']\n\n\nAge_Group\nAdults (35-64)          0.434241\nYoung Adults (25-34)    0.380264\nYouth (<25)             0.183072\nSeniors (64+)           0.002423\nName: Age_Group, dtype: float64\n\n\n\n\nCode\nsales.dtypes.value_counts()\n\n\nint64             9\nobject            8\ndatetime64[ns]    1\ndtype: int64"
  },
  {
    "objectID": "data_transformation.html#bar-graph",
    "href": "data_transformation.html#bar-graph",
    "title": "Data Transformation",
    "section": "Bar Graph",
    "text": "Bar Graph\n\n\nCode\npd.DataFrame(sales['Age_Group'].value_counts(normalize = True)).plot(kind = 'bar', figsize = (10,5))\n\n\n<Axes: >"
  },
  {
    "objectID": "data_transformation.html#correlations",
    "href": "data_transformation.html#correlations",
    "title": "Data Transformation",
    "section": "Correlations",
    "text": "Correlations\n\n\nCode\ncorr = sales.select_dtypes(include='int64').corr(numeric_only = False)\ncorr\n\n\n\n\n\n\n  \n    \n      \n      Day\n      Year\n      Customer_Age\n      Order_Quantity\n      Unit_Cost\n      Unit_Price\n      Profit\n      Cost\n      Revenue\n    \n  \n  \n    \n      Day\n      1.000000\n      -0.007635\n      -0.014296\n      -0.002412\n      0.003133\n      0.003207\n      0.004623\n      0.003329\n      0.003853\n    \n    \n      Year\n      -0.007635\n      1.000000\n      0.040994\n      0.123169\n      -0.217575\n      -0.213673\n      -0.181525\n      -0.215604\n      -0.208673\n    \n    \n      Customer_Age\n      -0.014296\n      0.040994\n      1.000000\n      0.026887\n      -0.021374\n      -0.020262\n      0.004319\n      -0.016013\n      -0.009326\n    \n    \n      Order_Quantity\n      -0.002412\n      0.123169\n      0.026887\n      1.000000\n      -0.515835\n      -0.515925\n      -0.238863\n      -0.340382\n      -0.312895\n    \n    \n      Unit_Cost\n      0.003133\n      -0.217575\n      -0.021374\n      -0.515835\n      1.000000\n      0.997894\n      0.741020\n      0.829869\n      0.817865\n    \n    \n      Unit_Price\n      0.003207\n      -0.213673\n      -0.020262\n      -0.515925\n      0.997894\n      1.000000\n      0.749870\n      0.826301\n      0.818522\n    \n    \n      Profit\n      0.004623\n      -0.181525\n      0.004319\n      -0.238863\n      0.741020\n      0.749870\n      1.000000\n      0.902233\n      0.956572\n    \n    \n      Cost\n      0.003329\n      -0.215604\n      -0.016013\n      -0.340382\n      0.829869\n      0.826301\n      0.902233\n      1.000000\n      0.988758\n    \n    \n      Revenue\n      0.003853\n      -0.208673\n      -0.009326\n      -0.312895\n      0.817865\n      0.818522\n      0.956572\n      0.988758\n      1.000000\n    \n  \n\n\n\n\n\n\nCode\npx.imshow(corr)"
  },
  {
    "objectID": "data_transformation.html#modifying-all",
    "href": "data_transformation.html#modifying-all",
    "title": "Data Transformation",
    "section": "Modifying All",
    "text": "Modifying All\n\n\nCode\nsales['Calculated_Cost'] *= 1.03\nsales['Calculated_Cost'].head()\n\n\n60993     354.32\n58729    4472.26\n58913    1564.57\n63711    2203.17\n49787    1289.56\nName: Calculated_Cost, dtype: float64"
  },
  {
    "objectID": "data_transformation.html#working-with-loc",
    "href": "data_transformation.html#working-with-loc",
    "title": "Data Transformation",
    "section": "Working with loc",
    "text": "Working with loc\n\n\nCode\nmarvel_df.loc['Vision', 'first_appearance'] = 1964\nmarvel_df\n\n\n\n\n\n\n  \n    \n      \n      name\n      sex\n      first_appearance\n    \n  \n  \n    \n      0\n      Spider-Man\n      male\n      1962.0\n    \n    \n      1\n      Captain America\n      male\n      1941.0\n    \n    \n      2\n      Wolverine\n      male\n      1974.0\n    \n    \n      3\n      Iron Man\n      male\n      1963.0\n    \n    \n      4\n      Thor\n      male\n      1963.0\n    \n    \n      5\n      Thing\n      male\n      1961.0\n    \n    \n      6\n      Mister Fantastic\n      male\n      1961.0\n    \n    \n      7\n      Hulk\n      male\n      1962.0\n    \n    \n      8\n      Beast\n      male\n      1963.0\n    \n    \n      9\n      Invisible Woman\n      female\n      1961.0\n    \n    \n      10\n      Storm\n      female\n      1975.0\n    \n    \n      11\n      Namor\n      male\n      1939.0\n    \n    \n      12\n      Hawkeye\n      male\n      1964.0\n    \n    \n      13\n      Daredevil\n      male\n      1964.0\n    \n    \n      14\n      Doctor Strange\n      male\n      1963.0\n    \n    \n      15\n      Hank Pym\n      male\n      1962.0\n    \n    \n      16\n      Scarlet Witch\n      female\n      1964.0\n    \n    \n      17\n      Wasp\n      female\n      1963.0\n    \n    \n      18\n      Black Widow\n      female\n      1964.0\n    \n    \n      19\n      Vision\n      male\n      1968.0\n    \n    \n      Vision\n      NaN\n      NaN\n      1964.0\n    \n  \n\n\n\n\n\n\nCode\nsales.loc[sales['Country'] == 'France', 'Revenue'].head()\n\n\n58729    5439\n58719    6226\n58721    9339\n58707    2934\n58727    3363\nName: Revenue, dtype: int64"
  },
  {
    "objectID": "data_transformation.html#long-to-wide",
    "href": "data_transformation.html#long-to-wide",
    "title": "Data Transformation",
    "section": "Long to Wide",
    "text": "Long to Wide\n\n\nCode\n# https://beta.bls.gov/dataQuery/find?fq=survey:[ap]&s=popularity:D\n# This data came from bls\ndf_long = pd.read_csv(\"data/file.csv\")\ndf_long\n\n\n\n\n\n\n  \n    \n      \n      Series ID\n      Item\n      Year Month\n      Avg. Price ($)\n    \n  \n  \n    \n      0\n      APU0000702111\n      Bread, whilte per lb\n      2020 Jan\n      1.351\n    \n    \n      1\n      APU0000702111\n      Bread, whilte per lb\n      2020 Feb\n      1.375\n    \n    \n      2\n      APU0000702111\n      Bread, whilte per lb\n      2020 Mar\n      1.374\n    \n    \n      3\n      APU0000702111\n      Bread, whilte per lb\n      2020 Apr\n      1.406\n    \n    \n      4\n      APU0000702111\n      Bread, whilte per lb\n      2020 May\n      1.412\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      97\n      APU0000709112\n      Milk, Whole per gal\n      2022 Jun\n      4.153\n    \n    \n      98\n      APU0000709112\n      Milk, Whole per gal\n      2022 Jul\n      4.156\n    \n    \n      99\n      APU0000709112\n      Milk, Whole per gal\n      2022 Aug\n      4.194\n    \n    \n      100\n      APU0000709112\n      Milk, Whole per gal\n      2022 Sep\n      4.181\n    \n    \n      101\n      APU0000709112\n      Milk, Whole per gal\n      2022 Oct\n      4.184\n    \n  \n\n102 rows × 4 columns\n\n\n\n\n\nCode\n# unmelting using pivot()\ndf_wide=pd.pivot(df_long, index=['Series ID','Item'], columns = 'Year Month',values = 'Avg. Price ($)') #Reshape from long to wide\n\ndf_wide.reset_index()\n\n\n\n\n\n\n  \n    \n      Year Month\n      Series ID\n      Item\n      2020 Apr\n      2020 Aug\n      2020 Dec\n      2020 Feb\n      2020 Jan\n      2020 Jul\n      2020 Jun\n      2020 Mar\n      ...\n      2022 Apr\n      2022 Aug\n      2022 Feb\n      2022 Jan\n      2022 Jul\n      2022 Jun\n      2022 Mar\n      2022 May\n      2022 Oct\n      2022 Sep\n    \n  \n  \n    \n      0\n      APU0000702111\n      Bread, whilte per lb\n      1.406\n      1.495\n      1.538\n      1.375\n      1.351\n      1.485\n      1.474\n      1.374\n      ...\n      1.612\n      1.756\n      1.578\n      1.555\n      1.715\n      1.691\n      1.607\n      1.606\n      1.814\n      1.749\n    \n    \n      1\n      APU0000708111\n      Eggs, large per doz\n      2.019\n      1.328\n      1.481\n      1.449\n      1.461\n      1.401\n      1.554\n      1.525\n      ...\n      2.520\n      3.116\n      2.005\n      1.929\n      2.936\n      2.707\n      2.046\n      2.863\n      3.419\n      2.902\n    \n    \n      2\n      APU0000709112\n      Milk, Whole per gal\n      3.267\n      3.406\n      3.535\n      3.196\n      3.253\n      3.255\n      3.198\n      3.248\n      ...\n      4.012\n      4.194\n      3.875\n      3.787\n      4.156\n      4.153\n      3.917\n      4.204\n      4.184\n      4.181\n    \n  \n\n3 rows × 36 columns"
  },
  {
    "objectID": "data_transformation.html#wide-to-long",
    "href": "data_transformation.html#wide-to-long",
    "title": "Data Transformation",
    "section": "Wide to Long",
    "text": "Wide to Long\n\n\nCode\nyear_list=list(df_wide.columns)\ndf_long = pd.melt(df_wide, value_vars=year_list,value_name='Avg. Price ($)', ignore_index=False).reset_index()\ndf_long\n\n\n\n\n\n\n  \n    \n      \n      Series ID\n      Item\n      Year Month\n      Avg. Price ($)\n    \n  \n  \n    \n      0\n      APU0000702111\n      Bread, whilte per lb\n      2020 Apr\n      1.406\n    \n    \n      1\n      APU0000708111\n      Eggs, large per doz\n      2020 Apr\n      2.019\n    \n    \n      2\n      APU0000709112\n      Milk, Whole per gal\n      2020 Apr\n      3.267\n    \n    \n      3\n      APU0000702111\n      Bread, whilte per lb\n      2020 Aug\n      1.495\n    \n    \n      4\n      APU0000708111\n      Eggs, large per doz\n      2020 Aug\n      1.328\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      97\n      APU0000708111\n      Eggs, large per doz\n      2022 Oct\n      3.419\n    \n    \n      98\n      APU0000709112\n      Milk, Whole per gal\n      2022 Oct\n      4.184\n    \n    \n      99\n      APU0000702111\n      Bread, whilte per lb\n      2022 Sep\n      1.749\n    \n    \n      100\n      APU0000708111\n      Eggs, large per doz\n      2022 Sep\n      2.902\n    \n    \n      101\n      APU0000709112\n      Milk, Whole per gal\n      2022 Sep\n      4.181\n    \n  \n\n102 rows × 4 columns"
  },
  {
    "objectID": "data_transformation.html#calculating-percentage-share",
    "href": "data_transformation.html#calculating-percentage-share",
    "title": "Data Transformation",
    "section": "Calculating Percentage Share",
    "text": "Calculating Percentage Share\n\n\nCode\nsales_yr_age = sales.groupby(['Year','Age_Group']).agg({'Profit':'sum','Revenue':'sum'})\nsales_yr_age['Revenue_Share'] = sales_yr_age['Revenue'] / sales_yr_age.groupby(['Year'])['Revenue'].sum()\nsales_yr_age['Profit_Share'] = sales_yr_age['Profit'] / sales_yr_age.groupby(['Year'])['Profit'].sum()\nsales_yr_age.reset_index()\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Age_Group\n      Profit\n      Revenue\n      Revenue_Share\n      Profit_Share\n    \n  \n  \n    \n      0\n      2011\n      Adults (35-64)\n      1278031\n      3964024.4\n      0.437552\n      0.443560\n    \n    \n      1\n      2011\n      Seniors (64+)\n      9169\n      25363.0\n      0.002800\n      0.003182\n    \n    \n      2\n      2011\n      Young Adults (25-34)\n      1085579\n      3448203.7\n      0.380615\n      0.376767\n    \n    \n      3\n      2011\n      Youth (<25)\n      508522\n      1621959.3\n      0.179033\n      0.176490\n    \n    \n      4\n      2012\n      Adults (35-64)\n      1336260\n      4144715.5\n      0.447005\n      0.452664\n    \n    \n      5\n      2012\n      Seniors (64+)\n      7506\n      23230.0\n      0.002505\n      0.002543\n    \n    \n      6\n      2012\n      Young Adults (25-34)\n      1108066\n      3503094.7\n      0.377806\n      0.375362\n    \n    \n      7\n      2012\n      Youth (<25)\n      500161\n      1601158.1\n      0.172684\n      0.169432\n    \n    \n      8\n      2013\n      Adults (35-64)\n      2737478\n      6959641.5\n      0.452418\n      0.459369\n    \n    \n      9\n      2013\n      Seniors (64+)\n      17764\n      39987.0\n      0.002599\n      0.002981\n    \n    \n      10\n      2013\n      Young Adults (25-34)\n      2252986\n      5951647.4\n      0.386892\n      0.378068\n    \n    \n      11\n      2013\n      Youth (<25)\n      950980\n      2431948.1\n      0.158091\n      0.159582\n    \n    \n      12\n      2014\n      Adults (35-64)\n      3416871\n      8319817.5\n      0.582056\n      0.582677\n    \n    \n      13\n      2014\n      Seniors (64+)\n      40309\n      81552.1\n      0.005705\n      0.006874\n    \n    \n      14\n      2014\n      Young Adults (25-34)\n      1839782\n      4544252.0\n      0.317917\n      0.313737\n    \n    \n      15\n      2014\n      Youth (<25)\n      567125\n      1348212.4\n      0.094321\n      0.096712\n    \n    \n      16\n      2015\n      Adults (35-64)\n      3436847\n      9111447.2\n      0.450799\n      0.456508\n    \n    \n      17\n      2015\n      Seniors (64+)\n      19994\n      47500.9\n      0.002350\n      0.002656\n    \n    \n      18\n      2015\n      Young Adults (25-34)\n      2880608\n      7869522.8\n      0.389353\n      0.382624\n    \n    \n      19\n      2015\n      Youth (<25)\n      1191114\n      3183312.9\n      0.157498\n      0.158213\n    \n    \n      20\n      2016\n      Adults (35-64)\n      4116095\n      10446662.3\n      0.583818\n      0.585009\n    \n    \n      21\n      2016\n      Seniors (64+)\n      43423\n      91806.9\n      0.005131\n      0.006172\n    \n    \n      22\n      2016\n      Young Adults (25-34)\n      2219740\n      5720280.6\n      0.319681\n      0.315486\n    \n    \n      23\n      2016\n      Youth (<25)\n      656690\n      1634954.9\n      0.091370\n      0.093334"
  },
  {
    "objectID": "data_transformation.html#aggregating-dates",
    "href": "data_transformation.html#aggregating-dates",
    "title": "Data Transformation",
    "section": "Aggregating Dates",
    "text": "Aggregating Dates\n\n\nCode\ndf_freq = pd.DataFrame({\n    \"Publish Date\" : [\n        pd.Timestamp(\"2000-01-01\"),\n        pd.Timestamp(\"2000-01-02\"),\n        pd.Timestamp(\"2000-01-02\"),\n        pd.Timestamp(\"2000-01-02\"),\n        pd.Timestamp(\"2000-01-09\"),\n        pd.Timestamp(\"2000-01-16\")\n    ],\n    \"ID\": [0, 1, 2, 3, 4, 5],\n    \"Price\": [10, 20, 30, 40, 50, 60]\n    }\n)\ndf_freq\n\n\n\n\n\n\n  \n    \n      \n      Publish Date\n      ID\n      Price\n    \n  \n  \n    \n      0\n      2000-01-01\n      0\n      10\n    \n    \n      1\n      2000-01-02\n      1\n      20\n    \n    \n      2\n      2000-01-02\n      2\n      30\n    \n    \n      3\n      2000-01-02\n      3\n      40\n    \n    \n      4\n      2000-01-09\n      4\n      50\n    \n    \n      5\n      2000-01-16\n      5\n      60\n    \n  \n\n\n\n\n\n\nCode\ndf_freq.groupby('Publish Date')['Price'].mean()\n\n\nPublish Date\n2000-01-01    10.0\n2000-01-02    30.0\n2000-01-09    50.0\n2000-01-16    60.0\nName: Price, dtype: float64"
  },
  {
    "objectID": "data_transformation.html#filling-gaps",
    "href": "data_transformation.html#filling-gaps",
    "title": "Data Transformation",
    "section": "Filling Gaps",
    "text": "Filling Gaps\n\n\nCode\ndf_freq_daily = df_freq.groupby(pd.Grouper(key = \"Publish Date\", freq= \"1D\"))['Price'].mean().reset_index()\ndf_freq_daily\n\n\n\n\n\n\n  \n    \n      \n      Publish Date\n      Price\n    \n  \n  \n    \n      0\n      2000-01-01\n      10.0\n    \n    \n      1\n      2000-01-02\n      30.0\n    \n    \n      2\n      2000-01-03\n      NaN\n    \n    \n      3\n      2000-01-04\n      NaN\n    \n    \n      4\n      2000-01-05\n      NaN\n    \n    \n      5\n      2000-01-06\n      NaN\n    \n    \n      6\n      2000-01-07\n      NaN\n    \n    \n      7\n      2000-01-08\n      NaN\n    \n    \n      8\n      2000-01-09\n      50.0\n    \n    \n      9\n      2000-01-10\n      NaN\n    \n    \n      10\n      2000-01-11\n      NaN\n    \n    \n      11\n      2000-01-12\n      NaN\n    \n    \n      12\n      2000-01-13\n      NaN\n    \n    \n      13\n      2000-01-14\n      NaN\n    \n    \n      14\n      2000-01-15\n      NaN\n    \n    \n      15\n      2000-01-16\n      60.0"
  },
  {
    "objectID": "data_transformation.html#counting-unique-values",
    "href": "data_transformation.html#counting-unique-values",
    "title": "Data Transformation",
    "section": "Counting Unique Values",
    "text": "Counting Unique Values\n\n\nCode\nsales.groupby('Age_Group').agg({'State':'nunique'})\n\n\n\n\n\n\n  \n    \n      \n      State\n    \n    \n      Age_Group\n      \n    \n  \n  \n    \n      Adults (35-64)\n      47\n    \n    \n      Seniors (64+)\n      18\n    \n    \n      Young Adults (25-34)\n      42\n    \n    \n      Youth (<25)\n      41"
  },
  {
    "objectID": "data_transformation.html#aggregating-on-multiple-values",
    "href": "data_transformation.html#aggregating-on-multiple-values",
    "title": "Data Transformation",
    "section": "Aggregating on Multiple Values",
    "text": "Aggregating on Multiple Values\n\n\nCode\nsales_country = sales.groupby('Country').agg({'Order_Quantity':np.sum,'Revenue':np.sum}).reset_index()\nsales_country\n\n\n\n\n\n\n  \n    \n      \n      Country\n      Order_Quantity\n      Revenue\n    \n  \n  \n    \n      0\n      Australia\n      263585\n      21302059.0\n    \n    \n      1\n      Canada\n      192259\n      7935738.0\n    \n    \n      2\n      France\n      128995\n      9276159.2\n    \n    \n      3\n      Germany\n      125720\n      8978596.0\n    \n    \n      4\n      United Kingdom\n      157218\n      10646196.0\n    \n    \n      5\n      United States\n      477539\n      27975547.0\n    \n  \n\n\n\n\n\n\nCode\nsales_country['aov'] = sales_country['Revenue']/ sales_country['Order_Quantity']\nsales_country\n\n\n\n\n\n\n  \n    \n      \n      Country\n      Order_Quantity\n      Revenue\n      aov\n    \n  \n  \n    \n      0\n      Australia\n      263585\n      21302059.0\n      80.816659\n    \n    \n      1\n      Canada\n      192259\n      7935738.0\n      41.276289\n    \n    \n      2\n      France\n      128995\n      9276159.2\n      71.910998\n    \n    \n      3\n      Germany\n      125720\n      8978596.0\n      71.417404\n    \n    \n      4\n      United Kingdom\n      157218\n      10646196.0\n      67.716139\n    \n    \n      5\n      United States\n      477539\n      27975547.0\n      58.582748"
  },
  {
    "objectID": "data_transformation.html#reset-index-alternative",
    "href": "data_transformation.html#reset-index-alternative",
    "title": "Data Transformation",
    "section": "Reset Index Alternative",
    "text": "Reset Index Alternative\n\n\nCode\n# USE as_index = False INSTEAD OF reset_index()\nsales.groupby('Country', as_index=False).agg(\n    Order_Total = (\"Order_Quantity\", np.sum),\n    Orders_Avg = (\"Order_Quantity\", np.mean),\n    Revenue_Total = (\"Revenue\", np.sum),\n    Revenue_avg = (\"Revenue\", np.mean),\n    States = (\"State\", np.count_nonzero),\n    Unique_States = (\"State\", pd.Series.nunique)\n)\n\n\n\n\n\n\n  \n    \n      \n      Country\n      Order_Total\n      Orders_Avg\n      Revenue_Total\n      Revenue_avg\n      States\n      Unique_States\n    \n  \n  \n    \n      0\n      Australia\n      263585\n      11.012074\n      21302059.0\n      889.959016\n      23936\n      5\n    \n    \n      1\n      Canada\n      192259\n      13.560375\n      7935738.0\n      559.721964\n      14178\n      3\n    \n    \n      2\n      France\n      128995\n      11.728951\n      9276159.2\n      843.440553\n      10998\n      16\n    \n    \n      3\n      Germany\n      125720\n      11.328167\n      8978596.0\n      809.028293\n      11098\n      6\n    \n    \n      4\n      United Kingdom\n      157218\n      11.543172\n      10646196.0\n      781.659031\n      13620\n      1\n    \n    \n      5\n      United States\n      477539\n      12.180253\n      27975547.0\n      713.552696\n      39206\n      22"
  },
  {
    "objectID": "data_transformation.html#weighted-average-calculation",
    "href": "data_transformation.html#weighted-average-calculation",
    "title": "Data Transformation",
    "section": "Weighted Average Calculation",
    "text": "Weighted Average Calculation\n\nBest\n\n\nCode\ncensus_wavg = census_county.groupby('state_name').agg(\n  population = ('population',np.sum),\n  weighted_avg=('median_income', lambda x: np.average(x, weights=census_county.loc[x.index, 'population']))\n  )\n\ncensus_wavg.reset_index().head()\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      population\n      weighted_avg\n    \n  \n  \n    \n      0\n      Alabama\n      43405190\n      45992.716379\n    \n    \n      1\n      Alaska\n      6544837\n      73239.142710\n    \n    \n      2\n      Arizona\n      59967006\n      52773.774548\n    \n    \n      3\n      Arkansas\n      26587370\n      43271.158347\n    \n    \n      4\n      California\n      344511143\n      66478.837893\n    \n  \n\n\n\n\n\n\nGreat\n\n\nCode\ncensus_wavg = census_county.groupby('state_name').apply(lambda x: np.average(x['median_income'], weights=x['population'])).reset_index()\ncensus_wavg = census_wavg.rename(columns = {0:'median_income'})\ncensus_wavg.head()\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      median_income\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n    \n    \n      1\n      Alaska\n      73239.142710\n    \n    \n      2\n      Arizona\n      52773.774548\n    \n    \n      3\n      Arkansas\n      43271.158347\n    \n    \n      4\n      California\n      66478.837893\n    \n  \n\n\n\n\n\n\nGood\n\n\nCode\n# define a custom function to calculate the weighted average\ndef weighted_average(df, value_column, weight_column):\n    return (df[value_column] * df[weight_column]).sum() / df[weight_column].sum()\n\n# group by the 'group' column and apply the custom function to calculate the weighted average within each group\ncensus_wavg = census_county.groupby('state_name').apply(\n  weighted_average, value_column='median_income', weight_column='population'\n  ).reset_index()\n  \ncensus_wavg = census_wavg.rename(columns = {0:'medain_income'}).head()\ncensus_wavg\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      medain_income\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n    \n    \n      1\n      Alaska\n      73239.142710\n    \n    \n      2\n      Arizona\n      52773.774548\n    \n    \n      3\n      Arkansas\n      43271.158347\n    \n    \n      4\n      California\n      66478.837893"
  },
  {
    "objectID": "data_transformation.html#single-filter",
    "href": "data_transformation.html#single-filter",
    "title": "Data Transformation",
    "section": "Single Filter",
    "text": "Single Filter\n\n\nCode\nsales = sales.set_index('Country')\nsales.loc['Canada'].head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Day\n      Month\n      Year\n      Customer_Age\n      Age_Group\n      Customer_Gender\n      State\n      Product_Category\n      Sub_Category\n      Product\n      Order_Quantity\n      Unit_Cost\n      Unit_Price\n      Profit\n      Cost\n      Revenue\n      Revenue_per_Age\n      Calculated_Cost\n    \n    \n      Country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Canada\n      2011-01-01\n      1\n      January\n      2011\n      17\n      Youth (<25)\n      M\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-250 Red, 44\n      1\n      1519\n      2443\n      900\n      1519\n      2419.0\n      142.294118\n      1564.57\n    \n    \n      Canada\n      2011-01-03\n      3\n      January\n      2011\n      45\n      Adults (35-64)\n      M\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-750 Black, 44\n      3\n      344\n      540\n      572\n      1032\n      1604.0\n      35.644444\n      1062.96\n    \n    \n      Canada\n      2011-01-05\n      5\n      January\n      2011\n      51\n      Adults (35-64)\n      M\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-650 Black, 60\n      3\n      487\n      783\n      865\n      1461\n      2326.0\n      45.607843\n      1504.83\n    \n    \n      Canada\n      2011-01-06\n      6\n      January\n      2011\n      48\n      Adults (35-64)\n      M\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-750 Black, 48\n      2\n      344\n      540\n      381\n      688\n      1069.0\n      22.270833\n      708.64\n    \n    \n      Canada\n      2011-01-06\n      6\n      January\n      2011\n      19\n      Youth (<25)\n      F\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-150 Red, 62\n      1\n      2171\n      3578\n      1371\n      2171\n      3542.0\n      186.421053\n      2236.13"
  },
  {
    "objectID": "data_transformation.html#filtering-multiple-values",
    "href": "data_transformation.html#filtering-multiple-values",
    "title": "Data Transformation",
    "section": "Filtering Multiple Values",
    "text": "Filtering Multiple Values\n\n\nCode\nsales = sales.reset_index()\ngeo_list = ['Canada','Australia','United States']\ngeo_filter = sales['Country'].isin(geo_list)\nsales[geo_filter].head()\n\n\n\n\n\n\n  \n    \n      \n      Country\n      Date\n      Day\n      Month\n      Year\n      Customer_Age\n      Age_Group\n      Customer_Gender\n      State\n      Product_Category\n      Sub_Category\n      Product\n      Order_Quantity\n      Unit_Cost\n      Unit_Price\n      Profit\n      Cost\n      Revenue\n      Revenue_per_Age\n      Calculated_Cost\n    \n  \n  \n    \n      0\n      United States\n      2011-01-01\n      1\n      January\n      2011\n      42\n      Adults (35-64)\n      M\n      California\n      Bikes\n      Road Bikes\n      Road-750 Black, 44\n      1\n      344\n      540\n      185\n      344\n      529.0\n      12.595238\n      354.32\n    \n    \n      2\n      Canada\n      2011-01-01\n      1\n      January\n      2011\n      17\n      Youth (<25)\n      M\n      British Columbia\n      Bikes\n      Road Bikes\n      Road-250 Red, 44\n      1\n      1519\n      2443\n      900\n      1519\n      2419.0\n      142.294118\n      1564.57\n    \n    \n      3\n      United States\n      2011-01-01\n      1\n      January\n      2011\n      39\n      Adults (35-64)\n      M\n      Washington\n      Bikes\n      Road Bikes\n      Road-550-W Yellow, 38\n      3\n      713\n      1120\n      482\n      2139\n      2621.0\n      67.205128\n      2203.17\n    \n    \n      4\n      Australia\n      2011-01-01\n      1\n      January\n      2011\n      23\n      Youth (<25)\n      M\n      Victoria\n      Bikes\n      Mountain Bikes\n      Mountain-200 Black, 46\n      1\n      1252\n      2295\n      561\n      1252\n      1813.0\n      78.826087\n      1289.56\n    \n    \n      6\n      Australia\n      2011-01-02\n      2\n      January\n      2011\n      40\n      Adults (35-64)\n      M\n      Victoria\n      Bikes\n      Road Bikes\n      Road-550-W Yellow, 40\n      4\n      713\n      1120\n      687\n      2852\n      3539.0\n      88.475000\n      2937.56"
  },
  {
    "objectID": "data_transformation.html#retrieving-single-and-multiple-columns",
    "href": "data_transformation.html#retrieving-single-and-multiple-columns",
    "title": "Data Transformation",
    "section": "Retrieving Single and Multiple columns",
    "text": "Retrieving Single and Multiple columns\n\nBest\n\n\nCode\nsales.loc[(sales['Month'] == 11) &\n          (sales['Year'] == 2013),\n          ['Age_Group','Revenue']\n].head()\n\n\n\n\n\n\n  \n    \n      \n      Age_Group\n      Revenue\n    \n  \n  \n  \n\n\n\n\n\n\nGood\n\n\nCode\nsales.query(\"State == 'Kentucky' and Year == 2014\")[['Year','Age_Group','Revenue']]\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Age_Group\n      Revenue\n    \n  \n  \n    \n      44668\n      2014\n      Adults (35-64)\n      567.0\n    \n    \n      44672\n      2014\n      Adults (35-64)\n      54.0\n    \n    \n      44706\n      2014\n      Adults (35-64)\n      914.0\n    \n    \n      44728\n      2014\n      Adults (35-64)\n      238.0"
  },
  {
    "objectID": "data_transformation.html#filtering-aggregating",
    "href": "data_transformation.html#filtering-aggregating",
    "title": "Data Transformation",
    "section": "Filtering + Aggregating",
    "text": "Filtering + Aggregating\nGet the mean revenue of the Adults (35-64) sales group\n\n\nCode\nsales.loc[sales['Age_Group']=='Adults (35-64)', 'Revenue'].mean()\n\n\n769.3162152479221\n\n\nHow Many Records belong to Age Group Youth (<25) or Adults 35-64?\n\n\nCode\nsales.loc[(sales['Age_Group'] == 'Youth (<25)') | (sales['Age_Group'] == 'Adults (35-64)')].shape[0]\n\n\n73652\n\n\nGet the mean revenue of the sales group Adults (35-64) in United States\n\n\nCode\nsales.loc[(sales['Age_Group'] == 'Adults (35-64)') & (sales['Country'] == 'United States'), 'Revenue'].mean()\n\n\n726.7260473588342"
  },
  {
    "objectID": "data_transformation.html#columns-upper-case",
    "href": "data_transformation.html#columns-upper-case",
    "title": "Data Transformation",
    "section": "Columns Upper Case",
    "text": "Columns Upper Case\n\n\nCode\ncountry_stat.rename(index=str.upper)\n\n\n\n\n\n\n  \n    \n      \n      Population\n      GDP\n      Surface Area\n      HDI\n      Continent\n      Language\n    \n  \n  \n    \n      CANADA\n      35.467\n      1785387\n      9984670\n      0.913\n      America\n      English\n    \n    \n      FRANCE\n      63.951\n      2833687\n      640679\n      0.888\n      Europe\n      English\n    \n    \n      GERMANY\n      80.940\n      3874437\n      357114\n      0.916\n      Europe\n      English\n    \n    \n      ITALY\n      60.665\n      2167744\n      301336\n      0.873\n      Europe\n      English\n    \n    \n      JAPAN\n      127.061\n      4602367\n      377930\n      0.891\n      Asia\n      English\n    \n    \n      UNITED KINGDOM\n      64.511\n      2950039\n      242495\n      0.907\n      Europe\n      English\n    \n    \n      UNITED STATES\n      318.523\n      17348075\n      9525067\n      0.915\n      America\n      English\n    \n  \n\n\n\n\n\n\nCode\ncountry_stat.rename(index=str.lower)\n\n\n\n\n\n\n  \n    \n      \n      Population\n      GDP\n      Surface Area\n      HDI\n      Continent\n      Language\n    \n  \n  \n    \n      canada\n      35.467\n      1785387\n      9984670\n      0.913\n      America\n      English\n    \n    \n      france\n      63.951\n      2833687\n      640679\n      0.888\n      Europe\n      English\n    \n    \n      germany\n      80.940\n      3874437\n      357114\n      0.916\n      Europe\n      English\n    \n    \n      italy\n      60.665\n      2167744\n      301336\n      0.873\n      Europe\n      English\n    \n    \n      japan\n      127.061\n      4602367\n      377930\n      0.891\n      Asia\n      English\n    \n    \n      united kingdom\n      64.511\n      2950039\n      242495\n      0.907\n      Europe\n      English\n    \n    \n      united states\n      318.523\n      17348075\n      9525067\n      0.915\n      America\n      English"
  },
  {
    "objectID": "data_transformation.html#apending-or-unioning-values",
    "href": "data_transformation.html#apending-or-unioning-values",
    "title": "Data Transformation",
    "section": "Apending or Unioning Values",
    "text": "Apending or Unioning Values\n\n\nCode\n# create two sample DataFrames\ndf1 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})\ndf2 = pd.DataFrame({'col1': [7, 8, 9], 'col2': [10, 11, 12]})\n\n# concatenate the DataFrames vertically\nresult = pd.concat([df1, df2])\n\nprint(result)\n\n\n   col1  col2\n0     1     4\n1     2     5\n2     3     6\n0     7    10\n1     8    11\n2     9    12"
  },
  {
    "objectID": "data_transformation.html#identifying-null-values",
    "href": "data_transformation.html#identifying-null-values",
    "title": "Data Transformation",
    "section": "Identifying Null values",
    "text": "Identifying Null values\n\n\nCode\npd.isnull(np.nan)\npd.isnull(None)\npd.isna(np.nan)\npd.notnull(None)\npd.isna(None)\n\n\nTrue\n\n\n\n\nCode\npd.isnull(pd.DataFrame({\n    'Column A': [1, np.nan, 7],\n    'Column B': [np.nan, 2, 3],\n    'Column C': [np.nan, 2, np.nan]\n}))\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n    \n  \n  \n    \n      0\n      False\n      True\n      True\n    \n    \n      1\n      True\n      False\n      False\n    \n    \n      2\n      False\n      False\n      True"
  },
  {
    "objectID": "data_transformation.html#aggregating-with-nulls",
    "href": "data_transformation.html#aggregating-with-nulls",
    "title": "Data Transformation",
    "section": "Aggregating with Nulls",
    "text": "Aggregating with Nulls\n\n\nCode\npd.Series([1, 2, np.nan]).count()\npd.Series([1, 2, np.nan]).sum()\npd.Series([2, 2, np.nan]).mean()\n\n\n2.0"
  },
  {
    "objectID": "data_transformation.html#null-values-in-a-series",
    "href": "data_transformation.html#null-values-in-a-series",
    "title": "Data Transformation",
    "section": "Null Values in a Series",
    "text": "Null Values in a Series\n\n\nCode\ns = pd.Series([1, 2, 3, np.nan, np.nan, 4])\npd.notnull(s)\n\n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool\n\n\n\n\nCode\npd.isnull(s)\n\n\n0    False\n1    False\n2    False\n3     True\n4     True\n5    False\ndtype: bool\n\n\n\n\nCode\ns.isnull()\n\n\n0    False\n1    False\n2    False\n3     True\n4     True\n5    False\ndtype: bool"
  },
  {
    "objectID": "data_transformation.html#removing-null-values",
    "href": "data_transformation.html#removing-null-values",
    "title": "Data Transformation",
    "section": "Removing Null Values",
    "text": "Removing Null Values\n\n\nCode\ns[s.notnull()]\n\n\n0    1.0\n1    2.0\n2    3.0\n5    4.0\ndtype: float64"
  },
  {
    "objectID": "data_transformation.html#removing-null-values-1",
    "href": "data_transformation.html#removing-null-values-1",
    "title": "Data Transformation",
    "section": "Removing Null Values",
    "text": "Removing Null Values\n\n\nCode\ns\ns.dropna()\n\n\n0    1.0\n1    2.0\n2    3.0\n5    4.0\ndtype: float64"
  },
  {
    "objectID": "data_transformation.html#dataframes-and-nulls",
    "href": "data_transformation.html#dataframes-and-nulls",
    "title": "Data Transformation",
    "section": "DataFrames and Nulls",
    "text": "DataFrames and Nulls\n\n\nCode\ndf_nulls = pd.DataFrame({\n    'Column A': [1, np.nan, 30, np.nan],\n    'Column B': [2, 8, 31, np.nan],\n    'Column C': [np.nan, 9, 32, 100],\n    'Column D': [5, 8, 34, 110],\n})\ndf_nulls\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n      Column D\n    \n  \n  \n    \n      0\n      1.0\n      2.0\n      NaN\n      5\n    \n    \n      1\n      NaN\n      8.0\n      9.0\n      8\n    \n    \n      2\n      30.0\n      31.0\n      32.0\n      34\n    \n    \n      3\n      NaN\n      NaN\n      100.0\n      110"
  },
  {
    "objectID": "data_transformation.html#aggregating-nulls-w-data-frames",
    "href": "data_transformation.html#aggregating-nulls-w-data-frames",
    "title": "Data Transformation",
    "section": "Aggregating Nulls w/ Data Frames",
    "text": "Aggregating Nulls w/ Data Frames\n\n\nCode\ndf_nulls.isnull().sum()\ndf_nulls.dropna()\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n      Column D\n    \n  \n  \n    \n      2\n      30.0\n      31.0\n      32.0\n      34"
  },
  {
    "objectID": "data_transformation.html#drop-entire-columns",
    "href": "data_transformation.html#drop-entire-columns",
    "title": "Data Transformation",
    "section": "Drop Entire Columns",
    "text": "Drop Entire Columns\nThe Only column that doesn’t have any NA’s is Column D\n\n\nCode\ndf_nulls.dropna(axis=1)\n\n\n\n\n\n\n  \n    \n      \n      Column D\n    \n  \n  \n    \n      0\n      5\n    \n    \n      1\n      8\n    \n    \n      2\n      34\n    \n    \n      3\n      110\n    \n  \n\n\n\n\nIn this case, any row or column that contains at least one null value will be dropped. Which can be, depending on the case, too extreme. You can control this behavior with the how parameter. Can be either 'any' or 'all':\n\n\nCode\ndf_nulls2 = pd.DataFrame({\n    'Column A': [1, np.nan, 30],\n    'Column B': [2, np.nan, 31],\n    'Column C': [np.nan, np.nan, 100]\n})\ndf_nulls2\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n    \n  \n  \n    \n      0\n      1.0\n      2.0\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      30.0\n      31.0\n      100.0\n    \n  \n\n\n\n\n\n\nCode\ndf_nulls2.dropna(how='all') # if all columns have NA then drop\ndf_nulls2.dropna(how = 'any') # default behavior\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n    \n  \n  \n    \n      2\n      30.0\n      31.0\n      100.0\n    \n  \n\n\n\n\n\n\nCode\ndf_nulls2.dropna(thresh=3) # if at least 3 values are NA\ndf_nulls2.dropna(thresh=3, axis='columns')\n\n\n\n\n\n\n  \n    \n      \n    \n  \n  \n    \n      0\n    \n    \n      1\n    \n    \n      2"
  },
  {
    "objectID": "data_transformation.html#forwards-filling",
    "href": "data_transformation.html#forwards-filling",
    "title": "Data Transformation",
    "section": "Forwards filling",
    "text": "Forwards filling\n\n\nCode\ndf_nulls2.fillna(method='ffill', axis=0)\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n    \n  \n  \n    \n      0\n      1.0\n      2.0\n      NaN\n    \n    \n      1\n      1.0\n      2.0\n      NaN\n    \n    \n      2\n      30.0\n      31.0\n      100.0\n    \n  \n\n\n\n\n##Backward filling\n\n\nCode\ndf_nulls2.fillna(method='bfill', axis=0)\n\n\n\n\n\n\n  \n    \n      \n      Column A\n      Column B\n      Column C\n    \n  \n  \n    \n      0\n      1.0\n      2.0\n      100.0\n    \n    \n      1\n      30.0\n      31.0\n      100.0\n    \n    \n      2\n      30.0\n      31.0\n      100.0\n    \n  \n\n\n\n\n\n\nCode\nrents = pd.read_csv('data/zillow_data.csv')\nrents.describe()\n\n\n\n\n\n\n  \n    \n      \n      avg_rents\n      avg_rents_yy\n    \n  \n  \n    \n      count\n      280.000000\n      206.000000\n    \n    \n      mean\n      2290.758144\n      0.122740\n    \n    \n      std\n      851.574880\n      0.115126\n    \n    \n      min\n      1001.440657\n      -0.142965\n    \n    \n      25%\n      1475.427448\n      0.045593\n    \n    \n      50%\n      2173.088152\n      0.110836\n    \n    \n      75%\n      3176.567808\n      0.159704\n    \n    \n      max\n      3992.811584\n      0.396998"
  },
  {
    "objectID": "data_transformation.html#missing-data-in-graphs",
    "href": "data_transformation.html#missing-data-in-graphs",
    "title": "Data Transformation",
    "section": "Missing Data in Graphs",
    "text": "Missing Data in Graphs\nWe can see in the above line that there is missing data. Let’s use interpolate\n\n\nCode\nimport plotly.express as px\npx.line(rents, x = 'rent_date', y = 'avg_rents', color='RegionName')\n\n### We can see in the above line that there is missing data. Let's use interpolate\n\n\n\n                                                \n\n\n\n\nCode\nrents[rents['RegionName'] == 'New York County'] = rents[rents['RegionName'] == 'New York County'].interpolate(method = 'linear')\npx.line(rents, x = 'rent_date', y = 'avg_rents', color='RegionName')"
  },
  {
    "objectID": "data_transformation.html#replace",
    "href": "data_transformation.html#replace",
    "title": "Data Transformation",
    "section": "Replace",
    "text": "Replace\n\n\nCode\ngender['Sex'].replace('D','F')\n\n\n0    M\n1    F\n2    F\n3    F\n4    ?\nName: Sex, dtype: object\n\n\n\n\nCode\ngender['Sex'].replace({'D': 'F', 'N': 'M'})\n\n\n0    M\n1    F\n2    F\n3    F\n4    ?\nName: Sex, dtype: object\n\n\nIf you have many columns to replace, you could apply it at “DataFrame level”:\n\n\nCode\ngender.replace({\n    'Sex': {'D': 'F','N': 'M'},\n    'Age': { 290: 29}\n})\n\n\n\n\n\n\n  \n    \n      \n      Sex\n      Age\n    \n  \n  \n    \n      0\n      M\n      29\n    \n    \n      1\n      F\n      30\n    \n    \n      2\n      F\n      24\n    \n    \n      3\n      F\n      29\n    \n    \n      4\n      ?\n      25"
  },
  {
    "objectID": "data_transformation.html#integers-downcasting",
    "href": "data_transformation.html#integers-downcasting",
    "title": "Data Transformation",
    "section": "Integers Downcasting",
    "text": "Integers Downcasting\nThis can help reduce the data size\n\nint8 can store integers form -128 to 127\nint16 can store integers from -32768 to 32767\nint64 can store integers from -9223372036854775808 to 9223372036854775807\n\n\n\nCode\nsales.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 113036 entries, 0 to 113035\nData columns (total 21 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   Country           113036 non-null  object        \n 1   Date              113036 non-null  datetime64[ns]\n 2   Day               113036 non-null  int64         \n 3   Month             113036 non-null  object        \n 4   Year              113036 non-null  int64         \n 5   Customer_Age      113036 non-null  int64         \n 6   Age_Group         113036 non-null  object        \n 7   Customer_Gender   113036 non-null  object        \n 8   State             113036 non-null  object        \n 9   Product_Category  113036 non-null  object        \n 10  Sub_Category      113036 non-null  object        \n 11  Product           113036 non-null  category      \n 12  Order_Quantity    113036 non-null  int64         \n 13  Unit_Cost         113036 non-null  int64         \n 14  Unit_Price        113036 non-null  int64         \n 15  Profit            113036 non-null  int64         \n 16  Cost              113036 non-null  int64         \n 17  Revenue           113036 non-null  float64       \n 18  Revenue_per_Age   113036 non-null  float64       \n 19  Calculated_Cost   113036 non-null  float64       \n 20  Year_Char         113036 non-null  object        \ndtypes: category(1), datetime64[ns](1), float64(3), int64(8), object(8)\nmemory usage: 17.5+ MB\n\n\n\n\nCode\nint8_dt = ['Day','Year','Customer_Age','Order_Quantity','Unit_Price','Profit','Cost']\nsales[int8_dt] = sales[int8_dt].astype('int8')\nsales.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 113036 entries, 0 to 113035\nData columns (total 21 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   Country           113036 non-null  object        \n 1   Date              113036 non-null  datetime64[ns]\n 2   Day               113036 non-null  int8          \n 3   Month             113036 non-null  object        \n 4   Year              113036 non-null  int8          \n 5   Customer_Age      113036 non-null  int8          \n 6   Age_Group         113036 non-null  object        \n 7   Customer_Gender   113036 non-null  object        \n 8   State             113036 non-null  object        \n 9   Product_Category  113036 non-null  object        \n 10  Sub_Category      113036 non-null  object        \n 11  Product           113036 non-null  category      \n 12  Order_Quantity    113036 non-null  int8          \n 13  Unit_Cost         113036 non-null  int64         \n 14  Unit_Price        113036 non-null  int8          \n 15  Profit            113036 non-null  int8          \n 16  Cost              113036 non-null  int8          \n 17  Revenue           113036 non-null  float64       \n 18  Revenue_per_Age   113036 non-null  float64       \n 19  Calculated_Cost   113036 non-null  float64       \n 20  Year_Char         113036 non-null  object        \ndtypes: category(1), datetime64[ns](1), float64(3), int64(1), int8(7), object(8)\nmemory usage: 12.2+ MB"
  },
  {
    "objectID": "data_transformation.html#automatically-convert-dtypes",
    "href": "data_transformation.html#automatically-convert-dtypes",
    "title": "Data Transformation",
    "section": "Automatically Convert Dtypes",
    "text": "Automatically Convert Dtypes\n\n\nCode\nsales.convert_dtypes().info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 113036 entries, 0 to 113035\nData columns (total 21 columns):\n #   Column            Non-Null Count   Dtype         \n---  ------            --------------   -----         \n 0   Country           113036 non-null  string        \n 1   Date              113036 non-null  datetime64[ns]\n 2   Day               113036 non-null  Int8          \n 3   Month             113036 non-null  string        \n 4   Year              113036 non-null  Int8          \n 5   Customer_Age      113036 non-null  Int8          \n 6   Age_Group         113036 non-null  string        \n 7   Customer_Gender   113036 non-null  string        \n 8   State             113036 non-null  string        \n 9   Product_Category  113036 non-null  string        \n 10  Sub_Category      113036 non-null  string        \n 11  Product           113036 non-null  category      \n 12  Order_Quantity    113036 non-null  Int8          \n 13  Unit_Cost         113036 non-null  Int64         \n 14  Unit_Price        113036 non-null  Int8          \n 15  Profit            113036 non-null  Int8          \n 16  Cost              113036 non-null  Int8          \n 17  Revenue           113036 non-null  Float64       \n 18  Revenue_per_Age   113036 non-null  Float64       \n 19  Calculated_Cost   113036 non-null  Float64       \n 20  Year_Char         113036 non-null  string        \ndtypes: Float64(3), Int64(1), Int8(7), category(1), datetime64[ns](1), string(8)\nmemory usage: 13.4 MB"
  },
  {
    "objectID": "data_transformation.html#select-int8-datatypes",
    "href": "data_transformation.html#select-int8-datatypes",
    "title": "Data Transformation",
    "section": "Select Int8 Datatypes",
    "text": "Select Int8 Datatypes\n\n\nCode\nsales.select_dtypes(include='int8').columns\n\n\nIndex(['Day', 'Year', 'Customer_Age', 'Order_Quantity', 'Unit_Price', 'Profit',\n       'Cost'],\n      dtype='object')"
  },
  {
    "objectID": "data_transformation.html#section",
    "href": "data_transformation.html#section",
    "title": "Data Transformation",
    "section": "",
    "text": "Code\ncensus_state[\"is_arizona\"] = np.where(census_state[\"state_name\"] == 'Arizona', True, False).copy()\ncensus_state\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      medain_income\n      population\n      is_arizona\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n      43405190\n      False\n    \n    \n      1\n      Alaska\n      73239.142710\n      6544837\n      False\n    \n    \n      2\n      Arizona\n      52773.774548\n      59967006\n      True\n    \n    \n      3\n      Arkansas\n      43271.158347\n      26587370\n      False\n    \n    \n      4\n      California\n      66478.837893\n      344511143\n      False"
  },
  {
    "objectID": "data_transformation.html#using-a-list",
    "href": "data_transformation.html#using-a-list",
    "title": "Data Transformation",
    "section": "Using A List",
    "text": "Using A List\n\n\nCode\ngeo_list =  ['Arizona','Nevada','New Mexico','Utah','Texas','Colorado']\ncensus_state[\"sun_belt\"] = np.where(census_state[\"state_name\"].isin(geo_list), True, False).copy()\ncensus_state\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      medain_income\n      population\n      is_arizona\n      sun_belt\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n      43405190\n      False\n      False\n    \n    \n      1\n      Alaska\n      73239.142710\n      6544837\n      False\n      False\n    \n    \n      2\n      Arizona\n      52773.774548\n      59967006\n      True\n      True\n    \n    \n      3\n      Arkansas\n      43271.158347\n      26587370\n      False\n      False\n    \n    \n      4\n      California\n      66478.837893\n      344511143\n      False\n      False"
  },
  {
    "objectID": "data_transformation.html#multiple-condition",
    "href": "data_transformation.html#multiple-condition",
    "title": "Data Transformation",
    "section": "Multiple Condition",
    "text": "Multiple Condition\n\n\nCode\ncensus_state[\"sun_belt\"] = np.where((census_state[\"state_name\"] == 'Arizona') |\n                                    (census_state[\"state_name\"] == 'Nevada') |\n                                    (census_state[\"state_name\"] == 'New Mexico'), True, False).copy()\ncensus_state\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      medain_income\n      population\n      is_arizona\n      sun_belt\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n      43405190\n      False\n      False\n    \n    \n      1\n      Alaska\n      73239.142710\n      6544837\n      False\n      False\n    \n    \n      2\n      Arizona\n      52773.774548\n      59967006\n      True\n      True\n    \n    \n      3\n      Arkansas\n      43271.158347\n      26587370\n      False\n      False\n    \n    \n      4\n      California\n      66478.837893\n      344511143\n      False\n      False"
  },
  {
    "objectID": "machine_learning_classification.html",
    "href": "machine_learning_classification.html",
    "title": "Classification",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "machine_learning_classification.html#option-1-iloc",
    "href": "machine_learning_classification.html#option-1-iloc",
    "title": "Classification",
    "section": "Option 1: iloc",
    "text": "Option 1: iloc\n\n\nCode\nX = dataset.iloc[ : , : -1].values\ny = dataset.iloc[ : , -1].values\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
  },
  {
    "objectID": "machine_learning_classification.html#option-2-not-iloc",
    "href": "machine_learning_classification.html#option-2-not-iloc",
    "title": "Classification",
    "section": "Option 2: Not iloc",
    "text": "Option 2: Not iloc\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n  dataset.drop(columns = 'Class'),\n  dataset.Class,\n  test_size = 0.25,\n  random_state = 0,\n  stratify = dataset.Class\n)"
  },
  {
    "objectID": "machine_learning_classification.html#predict",
    "href": "machine_learning_classification.html#predict",
    "title": "Classification",
    "section": "Predict",
    "text": "Predict\n\n\nCode\nX_const = sm.add_constant(X_test)\ny_pred = model.predict(X_const)\n\n\ny_pred[:5]\n\n# convert predicted values to binary labels (threshold=0.5)\ny_pred_bin = np.where(y_pred>0.5, 1, 0)\ny_pred_bin[:5]\n\n\narray([0, 1, 0, 0, 1])"
  },
  {
    "objectID": "machine_learning_classification.html#metrics",
    "href": "machine_learning_classification.html#metrics",
    "title": "Classification",
    "section": "Metrics",
    "text": "Metrics\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, classification_report\n\n\n\n\nCode\naccuracy = accuracy_score(y_test, y_pred_bin)\nrecall = recall_score(y_test, y_pred_bin) # use average=\"macro\" for multiclass problem \nprecision = precision_score(y_test, y_pred_bin) # use average=\"macro\" for multiclass problem \nf1 = f1_score(y_test, y_pred_bin) # use average=\"macro\" for multiclass problem\n\nprint('Accuracy Score: {:.2f}%'.format(accuracy*100))\nprint('Recall Score: {:.2f}%'.format(recall*100))\nprint('Precision Score: {:.2f}%'.format(precision*100))\nprint('F1 Score: {:.2f}%'.format(f1*100))\n\n\nAccuracy Score: 95.32%\nRecall Score: 91.67%\nPrecision Score: 94.83%\nF1 Score: 93.22%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred_bin))\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96       111\n           1       0.95      0.92      0.93        60\n\n    accuracy                           0.95       171\n   macro avg       0.95      0.94      0.95       171\nweighted avg       0.95      0.95      0.95       171"
  },
  {
    "objectID": "machine_learning_classification.html#prediction",
    "href": "machine_learning_classification.html#prediction",
    "title": "Classification",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\n# get the predicted probabilities from the model\ny_pred = result.predict(X)\n\n# convert the predicted probabilities to class labels using argmax\ny_pred = y_pred.argmax(axis=1)\n\ny_pred[:5]\n\n\narray([0, 0, 0, 0, 0])"
  },
  {
    "objectID": "machine_learning_classification.html#metrics-1",
    "href": "machine_learning_classification.html#metrics-1",
    "title": "Classification",
    "section": "Metrics",
    "text": "Metrics\n\n\nCode\naccuracy = accuracy_score(y, y_pred)\nrecall = recall_score(y, y_pred, average=\"macro\") # use average=\"macro\" for multiclass problem \nprecision = precision_score(y, y_pred, average=\"macro\") # use average=\"macro\" for multiclass problem \nf1 = f1_score(y, y_pred, average=\"macro\") # use average=\"macro\" for multiclass problem\n\nprint('Accuracy Score: {:.2f}%'.format(accuracy*100))\nprint('Recall Score: {:.2f}%'.format(recall*100))\nprint('Precision Score: {:.2f}%'.format(precision*100))\nprint('F1 Score: {:.2f}%'.format(f1*100))\n\n\nAccuracy Score: 98.67%\nRecall Score: 98.67%\nPrecision Score: 98.67%\nF1 Score: 98.67%\n\n\n\n\nCode\nprint(classification_report(y, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        50\n           1       0.98      0.98      0.98        50\n           2       0.98      0.98      0.98        50\n\n    accuracy                           0.99       150\n   macro avg       0.99      0.99      0.99       150\nweighted avg       0.99      0.99      0.99       150\n\n\n\nThis is to just prepare for the Ski-learn packages\n\n\nCode\ndataset = pd.read_csv('data/data_classification.csv')\nX_train, X_test, y_train, y_test = train_test_split(\n  dataset.drop(columns = 'Class'),\n  dataset.Class,\n  test_size = 0.25,\n  random_state = 0,\n  stratify = dataset.Class\n)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# map Class values to 0 and 1\ny_train = y_train.map({2:0, 4:1})\ny_test = y_test.map({2:0, 4:1})"
  },
  {
    "objectID": "machine_learning_classification.html#prediction-1",
    "href": "machine_learning_classification.html#prediction-1",
    "title": "Classification",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\ny_pred = classifier.predict(X_test)\ny_pred[:5]\n\n\narray([0, 1, 0, 0, 1])\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[108   3]\n [  5  55]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 95.32%\nRecall Score: 91.67%\nPrecision Score: 94.83%\nF1 Score: 93.22%\nROC AUC Score: 94.48%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96       111\n           1       0.95      0.92      0.93        60\n\n    accuracy                           0.95       171\n   macro avg       0.95      0.94      0.95       171\nweighted avg       0.95      0.95      0.95       171"
  },
  {
    "objectID": "machine_learning_classification.html#prediction-2",
    "href": "machine_learning_classification.html#prediction-2",
    "title": "Classification",
    "section": "Prediction",
    "text": "Prediction\n\n\nCode\ny_pred = classifier.predict(X_test)\nnp.set_printoptions(precision=2)\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[107   4]\n [  3  57]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 95.91%\nRecall Score: 95.00%\nPrecision Score: 93.44%\nF1 Score: 94.21%\nROC AUC Score: 95.70%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97       111\n           1       0.93      0.95      0.94        60\n\n    accuracy                           0.96       171\n   macro avg       0.95      0.96      0.96       171\nweighted avg       0.96      0.96      0.96       171"
  },
  {
    "objectID": "machine_learning_classification.html#linear-kernal",
    "href": "machine_learning_classification.html#linear-kernal",
    "title": "Classification",
    "section": "Linear Kernal",
    "text": "Linear Kernal\n\n\nCode\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear')\nclassifier.fit(X_train, y_train)\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\n\nPrediction\n\n\nCode\ny_pred = classifier.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[107   4]\n [  5  55]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 94.74%\nRecall Score: 91.67%\nPrecision Score: 93.22%\nF1 Score: 92.44%\nROC AUC Score: 94.03%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       111\n           1       0.93      0.92      0.92        60\n\n    accuracy                           0.95       171\n   macro avg       0.94      0.94      0.94       171\nweighted avg       0.95      0.95      0.95       171"
  },
  {
    "objectID": "machine_learning_classification.html#polynomial-kernal",
    "href": "machine_learning_classification.html#polynomial-kernal",
    "title": "Classification",
    "section": "Polynomial Kernal",
    "text": "Polynomial Kernal\n\n\nCode\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'poly')\nclassifier.fit(X_train, y_train)\n\n\nSVC(kernel='poly')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='poly')\n\n\n\nPrediction\n\n\nCode\ny_pred = classifier.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[110   1]\n [  9  51]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 94.15%\nRecall Score: 85.00%\nPrecision Score: 98.08%\nF1 Score: 91.07%\nROC AUC Score: 92.05%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.99      0.96       111\n           1       0.98      0.85      0.91        60\n\n    accuracy                           0.94       171\n   macro avg       0.95      0.92      0.93       171\nweighted avg       0.94      0.94      0.94       171"
  },
  {
    "objectID": "machine_learning_classification.html#radial-basis-function-rbf-kernal",
    "href": "machine_learning_classification.html#radial-basis-function-rbf-kernal",
    "title": "Classification",
    "section": "Radial Basis Function (RBF) Kernal",
    "text": "Radial Basis Function (RBF) Kernal\n\n\nCode\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\nPrediction\n\n\nCode\ny_pred = classifier.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[107   4]\n [  3  57]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 95.91%\nRecall Score: 95.00%\nPrecision Score: 93.44%\nF1 Score: 94.21%\nROC AUC Score: 95.70%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97       111\n           1       0.93      0.95      0.94        60\n\n    accuracy                           0.96       171\n   macro avg       0.95      0.96      0.96       171\nweighted avg       0.96      0.96      0.96       171"
  },
  {
    "objectID": "machine_learning_classification.html#sigmoid-kernal",
    "href": "machine_learning_classification.html#sigmoid-kernal",
    "title": "Classification",
    "section": "Sigmoid Kernal",
    "text": "Sigmoid Kernal\n\n\nCode\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'sigmoid')\nclassifier.fit(X_train, y_train)\n\n\nSVC(kernel='sigmoid')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='sigmoid')\n\n\n\nPrediction\n\n\nCode\ny_pred = classifier.predict(X_test)\n\n\n\n\nModel Performance\n\n\nCode\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n\n[[107   4]\n [  6  54]]\n\n\n\n\nCode\nprint(\"Accuracy Score: {:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\nprint(\"Recall Score: {:.2f}%\".format(recall_score(y_test, y_pred)*100))\nprint(\"Precision Score: {:.2f}%\".format(precision_score(y_test, y_pred)*100))\nprint(\"F1 Score: {:.2f}%\".format(f1_score(y_test, y_pred)*100))\nprint(\"ROC AUC Score: {:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))\n\n\nAccuracy Score: 94.15%\nRecall Score: 90.00%\nPrecision Score: 93.10%\nF1 Score: 91.53%\nROC AUC Score: 93.20%\n\n\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96       111\n           1       0.93      0.90      0.92        60\n\n    accuracy                           0.94       171\n   macro avg       0.94      0.93      0.94       171\nweighted avg       0.94      0.94      0.94       171"
  },
  {
    "objectID": "machine_learning_classification.html#accuracy",
    "href": "machine_learning_classification.html#accuracy",
    "title": "Classification",
    "section": "Accuracy",
    "text": "Accuracy\n\n\nCode\nscores = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, scoring='accuracy')\nprint(\"Accuracy Score: {:.2f}%\".format(scores.mean()*100))\nprint(\"Standard Deviation: {:.2f}%\".format(scores.std()*100))\n\n\nAccuracy Score: 97.08%\nStandard Deviation: 2.34%"
  },
  {
    "objectID": "machine_learning_classification.html#recall",
    "href": "machine_learning_classification.html#recall",
    "title": "Classification",
    "section": "Recall",
    "text": "Recall\n\n\nCode\nscores = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, scoring='recall')\nprint(\"Recall Score: {:.2f}%\".format(scores.mean()*100))\nprint(\"Standard Deviation: {:.2f}%\".format(scores.std()*100))\n\n\nRecall Score: 97.78%\nStandard Deviation: 3.69%"
  },
  {
    "objectID": "machine_learning_classification.html#f1-score",
    "href": "machine_learning_classification.html#f1-score",
    "title": "Classification",
    "section": "F1 Score",
    "text": "F1 Score\n\n\nCode\nscores = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, scoring='f1')\nprint(\"F1 Score: {:.2f}%\".format(scores.mean()*100))\nprint(\"Standard Deviation: {:.2f}%\".format(scores.std()*100))\n\n\nF1 Score: 95.91%\nStandard Deviation: 3.28%"
  },
  {
    "objectID": "machine_learning_classification.html#applying-results",
    "href": "machine_learning_classification.html#applying-results",
    "title": "Classification",
    "section": "Applying Results",
    "text": "Applying Results\n\nOption 1\n\n\nCode\nbest_svr = grid_search.best_estimator_\nbest_svr.fit(X_train,y_train)\n\ny_pred = best_svr.predict(X_test)\n\ny_pred[:5]\n\n\narray([0, 1, 0, 0, 1])\n\n\n\n\nOption 2\n\n\nCode\nclassifier = SVC(kernel = 'rbf', C = 1, gamma = 0.4)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\ny_pred[:5]\n\n\narray([0, 1, 0, 0, 1])"
  },
  {
    "objectID": "data_transformation.html#lag-function",
    "href": "data_transformation.html#lag-function",
    "title": "Data Transformation",
    "section": "Lag Function",
    "text": "Lag Function\n\n\nCode\nsales_by_day = sales.groupby('Date')['Calculated_Cost'].sum().reset_index()\n\nsales_by_day['CostGrowth'] = sales_by_day.Calculated_Cost / sales_by_day.Calculated_Cost.shift(1) - 1\nsales_by_day.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Calculated_Cost\n      CostGrowth\n    \n  \n  \n    \n      0\n      2011-01-01\n      9596\n      NaN\n    \n    \n      1\n      2011-01-02\n      8943\n      -0.068049\n    \n    \n      2\n      2011-01-03\n      19001\n      1.124679\n    \n    \n      3\n      2011-01-04\n      12669\n      -0.333246\n    \n    \n      4\n      2011-01-05\n      3031\n      -0.760755"
  },
  {
    "objectID": "data_transformation.html#ordinal-sorting",
    "href": "data_transformation.html#ordinal-sorting",
    "title": "Data Transformation",
    "section": "Ordinal Sorting",
    "text": "Ordinal Sorting\n\n\nCode\n# define the categories and their order\ncat_type = pd.CategoricalDtype(categories=['United States', 'Canada', 'United Kingdom','Germany',  'Australia', 'France' ], ordered=True)\n\n# change the datatype of the column to categorical\nsales[\"Country\"] = sales[\"Country\"].astype(cat_type)\n\n# check the datatype of the column again\nsales[\"Country\"].dtype\n\nsales.groupby('Country')['Revenue_per_Age'].sum()\n\n\nCountry\nUnited States     808700.750985\nCanada            235970.354014\nUnited Kingdom    328024.724224\nGermany           279645.607207\nAustralia         687907.908809\nFrance            260595.985085\nName: Revenue_per_Age, dtype: float64\n\n\n#Joining Data\n\n\nCode\ncensus_state = census_county.groupby('state_name').agg({'population':np.sum}).reset_index()\ncensus_state = pd.merge(census_wavg,\n                        census_state,\n                           how=\"left\",\n                           left_on = [\"state_name\"],\n                           right_on = [\"state_name\"])\ncensus_state.head()\n\n\n\n\n\n\n  \n    \n      \n      state_name\n      medain_income\n      population\n    \n  \n  \n    \n      0\n      Alabama\n      45992.716379\n      43405190\n    \n    \n      1\n      Alaska\n      73239.142710\n      6544837\n    \n    \n      2\n      Arizona\n      52773.774548\n      59967006\n    \n    \n      3\n      Arkansas\n      43271.158347\n      26587370\n    \n    \n      4\n      California\n      66478.837893\n      344511143"
  },
  {
    "objectID": "data_transformation.html#rolling-function",
    "href": "data_transformation.html#rolling-function",
    "title": "Data Transformation",
    "section": "Rolling Function",
    "text": "Rolling Function\n\n\nCode\n# Calculate the rolling mean of column A with a window size of 2\nsales_by_day['rolling_mean'] = sales_by_day['Calculated_Cost'].rolling(window=3).mean()\nsales_by_day['rolling_total'] = sales_by_day['Calculated_Cost'].rolling(window=3).sum()\nsales_by_day.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      Calculated_Cost\n      CostGrowth\n      rolling_mean\n      rolling_total\n    \n  \n  \n    \n      0\n      2011-01-01\n      9596\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      2011-01-02\n      8943\n      -0.068049\n      NaN\n      NaN\n    \n    \n      2\n      2011-01-03\n      19001\n      1.124679\n      12513.333333\n      37540.0\n    \n    \n      3\n      2011-01-04\n      12669\n      -0.333246\n      13537.666667\n      40613.0\n    \n    \n      4\n      2011-01-05\n      3031\n      -0.760755\n      11567.000000\n      34701.0"
  },
  {
    "objectID": "data_transformation.html#window-function",
    "href": "data_transformation.html#window-function",
    "title": "Data Transformation",
    "section": "Window Function",
    "text": "Window Function\n\nPercentage Share\n\nBest\n\n\nCode\n# Group the data by state and calculate the sum of the order quantity for each state\nsales_stacked_ratio = sales.groupby(['State','Age_Group'])['Order_Quantity'].sum().reset_index()\n\n# Divide the Order Quantity by the state total to get the percentage of the order quantity for each age group in each state\nsales_stacked_ratio['Percent_Weight'] = sales_stacked_ratio['Order_Quantity']/ sales_stacked_ratio.groupby('State')['Order_Quantity'].transform('sum')\n\nsales_stacked_ratio\n\n\n\n\n\n\n  \n    \n      \n      State\n      Age_Group\n      Order_Quantity\n      Percent_Weight\n    \n  \n  \n    \n      0\n      Alabama\n      Adults (35-64)\n      105\n      1.000000\n    \n    \n      1\n      Alberta\n      Adults (35-64)\n      357\n      0.608177\n    \n    \n      2\n      Alberta\n      Young Adults (25-34)\n      90\n      0.153322\n    \n    \n      3\n      Alberta\n      Youth (<25)\n      140\n      0.238501\n    \n    \n      4\n      Arizona\n      Young Adults (25-34)\n      4\n      0.571429\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      143\n      Wyoming\n      Young Adults (25-34)\n      84\n      1.000000\n    \n    \n      144\n      Yveline\n      Adults (35-64)\n      4355\n      0.399248\n    \n    \n      145\n      Yveline\n      Seniors (64+)\n      405\n      0.037129\n    \n    \n      146\n      Yveline\n      Young Adults (25-34)\n      4105\n      0.376329\n    \n    \n      147\n      Yveline\n      Youth (<25)\n      2043\n      0.187294\n    \n  \n\n148 rows × 4 columns\n\n\n\n\n\nGood\n\n\nCode\n# Group the data by state and calculate the sum of the order quantity for each state\nsales_stacked_ratio = sales.groupby(['State','Age_Group'])['Order_Quantity'].sum().reset_index()\nsales_totals = sales_stacked_ratio.groupby('State')['Order_Quantity'].sum()\n\n# Divide the Order Quantity by the state total to get the percentage of the order quantity for each age group in each state\nsales_stacked_ratio['Percent_Weight'] = sales_stacked_ratio['Order_Quantity'] / sales_stacked_ratio['State'].map(sales_totals)\n\nsales_stacked_ratio\n\n\n\n\n\n\n  \n    \n      \n      State\n      Age_Group\n      Order_Quantity\n      Percent_Weight\n    \n  \n  \n    \n      0\n      Alabama\n      Adults (35-64)\n      105\n      1.000000\n    \n    \n      1\n      Alberta\n      Adults (35-64)\n      357\n      0.608177\n    \n    \n      2\n      Alberta\n      Young Adults (25-34)\n      90\n      0.153322\n    \n    \n      3\n      Alberta\n      Youth (<25)\n      140\n      0.238501\n    \n    \n      4\n      Arizona\n      Young Adults (25-34)\n      4\n      0.571429\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      143\n      Wyoming\n      Young Adults (25-34)\n      84\n      1.000000\n    \n    \n      144\n      Yveline\n      Adults (35-64)\n      4355\n      0.399248\n    \n    \n      145\n      Yveline\n      Seniors (64+)\n      405\n      0.037129\n    \n    \n      146\n      Yveline\n      Young Adults (25-34)\n      4105\n      0.376329\n    \n    \n      147\n      Yveline\n      Youth (<25)\n      2043\n      0.187294\n    \n  \n\n148 rows × 4 columns"
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Code\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "data_visualization.html#stacked-bar-chart",
    "href": "data_visualization.html#stacked-bar-chart",
    "title": "Data Visualization",
    "section": "Stacked Bar Chart",
    "text": "Stacked Bar Chart\n\n\nCode\nfig = px.bar(sales_stacked_ratio, x=\"State\", y=\"Percent_Weight\", color=\"Age_Group\", \n            title=\"Age Group Share by State\")"
  },
  {
    "objectID": "data_timeseries.html",
    "href": "data_timeseries.html",
    "title": "Data Time Series",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "data_timeseries.html#setting-index",
    "href": "data_timeseries.html#setting-index",
    "title": "Data Time Series",
    "section": "Setting Index",
    "text": "Setting Index\n\n\nCode\nbtc_price = pd.read_csv(\n    'data/btc-market-price.csv',\n    header=None,\n    names=['Timestamp', 'Price'],\n    index_col=0, # this sets the column in this instance its the first column 1\n    # index_col='Timestamp',  # You can also add the column name\n    parse_dates=True\n)\n\nbtc_price.head()\n\n\n\n\n\n\n  \n    \n      \n      Price\n    \n    \n      Timestamp\n      \n    \n  \n  \n    \n      2017-04-02\n      1099.169125\n    \n    \n      2017-04-03\n      1141.813000\n    \n    \n      2017-04-04\n      1141.600363\n    \n    \n      2017-04-05\n      1133.079314\n    \n    \n      2017-04-06\n      1196.307937"
  },
  {
    "objectID": "data_timeseries.html#not-setting-index",
    "href": "data_timeseries.html#not-setting-index",
    "title": "Data Time Series",
    "section": "Not Setting Index",
    "text": "Not Setting Index\n\n\nCode\nbtc_price = pd.read_csv(\n    'data/btc-market-price.csv',\n    header=None,\n    names=['Timestamp', 'Price'],\n    parse_dates=True\n)\n\nbtc_price.head()\n\n\n\n\n\n\n  \n    \n      \n      Timestamp\n      Price\n    \n  \n  \n    \n      0\n      2017-04-02 00:00:00\n      1099.169125\n    \n    \n      1\n      2017-04-03 00:00:00\n      1141.813000\n    \n    \n      2\n      2017-04-04 00:00:00\n      1141.600363\n    \n    \n      3\n      2017-04-05 00:00:00\n      1133.079314\n    \n    \n      4\n      2017-04-06 00:00:00\n      1196.307937"
  },
  {
    "objectID": "data_timeseries.html#sorting-dates",
    "href": "data_timeseries.html#sorting-dates",
    "title": "Data Time Series",
    "section": "Sorting Dates",
    "text": "Sorting Dates\n\n\nCode\nsales = pd.read_csv(\n    'data/sales_data.csv',\n    parse_dates=['Date']\n)\n\nsales = sales.sort_values(by=['Date'], ascending = True)"
  },
  {
    "objectID": "data_timeseries.html#option-1-best",
    "href": "data_timeseries.html#option-1-best",
    "title": "Data Time Series",
    "section": "Option 1: Best",
    "text": "Option 1: Best\n\n\nCode\nstart_date = '2017-09-29'\nend_date = '2017-10-05'\n\nbtc_price.loc[(btc_price['Timestamp'] >= start_date) & (btc_price['Timestamp'] <= end_date)]\n\n\n\n\n\n\n  \n    \n      \n      index\n      Timestamp\n      Price\n    \n  \n  \n    \n      180\n      180\n      2017-09-29\n      4193.574667\n    \n    \n      181\n      181\n      2017-09-30\n      4335.368317\n    \n    \n      182\n      182\n      2017-10-01\n      4360.722967\n    \n    \n      183\n      183\n      2017-10-02\n      4386.883750\n    \n    \n      184\n      184\n      2017-10-03\n      4293.306600\n    \n    \n      185\n      185\n      2017-10-04\n      4225.175000\n    \n    \n      186\n      186\n      2017-10-05\n      4338.852000"
  },
  {
    "objectID": "data_timeseries.html#option-2-good",
    "href": "data_timeseries.html#option-2-good",
    "title": "Data Time Series",
    "section": "Option 2: Good",
    "text": "Option 2: Good\n\n\nCode\nbtc_price.set_index('Timestamp', inplace=True)\nbtc_price.loc['2017-09-29':'2017-10-05']\n\n\n\n\n\n\n  \n    \n      \n      index\n      Price\n    \n    \n      Timestamp\n      \n      \n    \n  \n  \n    \n      2017-09-29\n      180\n      4193.574667\n    \n    \n      2017-09-30\n      181\n      4335.368317\n    \n    \n      2017-10-01\n      182\n      4360.722967\n    \n    \n      2017-10-02\n      183\n      4386.883750\n    \n    \n      2017-10-03\n      184\n      4293.306600\n    \n    \n      2017-10-04\n      185\n      4225.175000\n    \n    \n      2017-10-05\n      186\n      4338.852000\n    \n  \n\n\n\n\n\n\nCode\nbtc_price = btc_price.reset_index()"
  },
  {
    "objectID": "data_transformation.html#adding-names",
    "href": "data_transformation.html#adding-names",
    "title": "Data Transformation",
    "section": "Adding Names",
    "text": "Adding Names\nAdding Names when ingesting and setting the index wheen reading in a csv\n\n\nCode\nbtc_price = pd.read_csv(\n    'data/btc-market-price.csv',\n    header=None,\n    names=['Timestamp', 'Price'],\n    index_col=0, # this sets the column in this instance its the first column 1\n    # index_col='Timestamp',  # You can also add the column name\n    parse_dates=True\n)\n\nbtc_price.head()\n\n\n\n\n\n\n  \n    \n      \n      Price\n    \n    \n      Timestamp\n      \n    \n  \n  \n    \n      2017-04-02\n      1099.169125\n    \n    \n      2017-04-03\n      1141.813000\n    \n    \n      2017-04-04\n      1141.600363\n    \n    \n      2017-04-05\n      1133.079314\n    \n    \n      2017-04-06\n      1196.307937"
  },
  {
    "objectID": "data_transformation.html#modifying-multiple-columns",
    "href": "data_transformation.html#modifying-multiple-columns",
    "title": "Data Transformation",
    "section": "Modifying Multiple Columns",
    "text": "Modifying Multiple Columns\n\n\nCode\ncrisis = pd.Series([-1_000_000, -0.3], index=['GDP', 'HDI'])\ncrisis\n\n\nGDP   -1000000.0\nHDI         -0.3\ndtype: float64\n\n\n\n\nCode\ncrisis[['GDP', 'HDI']] + crisis\n\n\nGDP   -2000000.0\nHDI         -0.6\ndtype: float64"
  },
  {
    "objectID": "data_transformation.html#working-with-iloc",
    "href": "data_transformation.html#working-with-iloc",
    "title": "Data Transformation",
    "section": "Working with iloc",
    "text": "Working with iloc\n\n\nCode\n# Create a DataFrame\ndf_loc = pd.DataFrame({'name': ['John', 'Mike', 'Sally', 'Jane'],\n                   'age': [25, 30, 35, 40],\n                   'gender': ['M', 'M', 'F', 'F']})\n\n# Select rows and columns using iloc\ndf_loc.iloc[1:3, 0:2]\n\n\n\n\n\n\n  \n    \n      \n      name\n      age\n    \n  \n  \n    \n      1\n      Mike\n      30\n    \n    \n      2\n      Sally\n      35\n    \n  \n\n\n\n\n\n\nCode\n# Select rows and columns using loc\ndf_loc.loc[df_loc['age'] > 30, ['name', 'gender']]\n\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n    \n  \n  \n    \n      2\n      Sally\n      F\n    \n    \n      3\n      Jane\n      F\n    \n  \n\n\n\n\n\n\nCode\nsales.loc[sales['Country'] == 'France', 'Revenue'] *= 1.10\nsales.loc[sales['Country'] == 'France', 'Revenue'].head()\n\n\n58729     5982.9\n58719     6848.6\n58721    10272.9\n58707     3227.4\n58727     3699.3\nName: Revenue, dtype: float64"
  },
  {
    "objectID": "data_visualization.html#pearson",
    "href": "data_visualization.html#pearson",
    "title": "Data Visualization",
    "section": "Pearson",
    "text": "Pearson\n\n\nCode\npx.scatter(df, x = 'days', y='values_linear')\n\n\n\n                                                \n\n\n\n\nCode\npearson_correlation = df.corr()['days']['values_linear']\nspearman_correlation = df.corr(method='spearman')['days']['values_linear']\n\nprint(\"Pearson Correlation: {:.2f}%\".format(pearson_correlation*100))\nprint(\"Spearman Correlation: {:.2f}%\".format(spearman_correlation*100))\n\n\nPearson Correlation: 88.37%\nSpearman Correlation: 87.96%"
  },
  {
    "objectID": "data_visualization.html#spearman",
    "href": "data_visualization.html#spearman",
    "title": "Data Visualization",
    "section": "Spearman",
    "text": "Spearman\n\n\nCode\n# fit polynomial regression model\nz = np.polyfit(x, df['values_linear'], 100)\nf = np.poly1d(z)\n\n# evaluate polynomial at each value of x\ndf['values_polynomial'] = np.polyval(f, x)\ndf['values_polynomial'] = df['values_polynomial'] * df['values_linear']\ndf['values_polynomial'] = df['values_polynomial']\n\n\n/Users/dancaley/Library/Python/3.9/lib/python/site-packages/numpy/lib/polynomial.py:666: RuntimeWarning:\n\noverflow encountered in multiply\n\n/Library/Python/3.9/site-packages/IPython/core/interactiveshell.py:3433: RankWarning:\n\nPolyfit may be poorly conditioned\n\n\n\n\n\nCode\npx.scatter(df, x = 'days', y='values_polynomial')\n\n\n\n                                                \n\n\n\n\nCode\npearson_correlation = df.corr()['days']['values_polynomial']\nspearman_correlation = df.corr(method='spearman')['days']['values_polynomial']\n\nprint(\"Pearson Correlation: {:.2f}%\".format(pearson_correlation*100))\nprint(\"Spearman Correlation: {:.2f}%\".format(spearman_correlation*100))\n\n\nPearson Correlation: 92.80%\nSpearman Correlation: 94.54%"
  },
  {
    "objectID": "data_visualization.html#correlation-plot",
    "href": "data_visualization.html#correlation-plot",
    "title": "Data Visualization",
    "section": "Correlation Plot",
    "text": "Correlation Plot\n\n\nCode\ncorr = sales.select_dtypes(include='int64').corr(numeric_only = False)\npx.imshow(corr)"
  },
  {
    "objectID": "data_transformation.html#reading-all",
    "href": "data_transformation.html#reading-all",
    "title": "Data Transformation",
    "section": "Reading All",
    "text": "Reading All\n```{python}\n\npd.read_csv(filename) # From a CSV file\npd.read_table(filename) # From a delimited text file (like TSV)\npd.read_excel(filename) # From an Excel file\npd.read_sql(query, connection_object) # Reads from a SQL table/database\npd.read_json(json_string) # Reads from a JSON formatted string, URL or file.\npd.read_html(url) # Parses an html URL, string or file and extracts tables to a list of dataframes\npd.read_clipboard() # Takes the contents of your clipboard and passes it to read_table()\npd.DataFrame(dict) # From a dict, keys for columns names, values for data as lists\n\n```\n```{python}\n\ndf.to_csv(filename) # Writes to a CSV file\ndf.to_excel(filename) # Writes to an Excel file\ndf.to_sql(table_name, connection_object) # Writes to a SQL table\ndf.to_json(filename) # Writes to a file in JSON format\ndf.to_html(filename) # Saves as an HTML table\ndf.to_clipboard() # Writes to the clipboard\n\n```"
  }
]